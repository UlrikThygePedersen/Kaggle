{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Titanic - Machine Learning from Disaster\n\n![](https://upload.wikimedia.org/wikipedia/commons/6/6e/St%C3%B6wer_Titanic.jpg)\n\n***The RMS Titanic sank in the early morning hours of 15 April 1912 in the North Atlantic Ocean, four days into her maiden voyage from Southampton to New York City. The largest ocean liner in service at the time, Titanic had an estimated 2,224 people on board when she struck an iceberg at around 23:40 (ship's time) on Sunday, 14 April 1912. Her sinking two hours and forty minutes later at 02:20 (ship's time; 05:18 GMT) on Monday, 15 April, resulted in the deaths of more than 1,500 people, making it one of the deadliest peacetime maritime disasters in history.*** ","metadata":{}},{"cell_type":"markdown","source":"# Table of Contents\n\n* [Introduction](#introduction)\n* [House Keeping](#house)\n* [Exploratory Data Analysis](#EDA)\n* [Feature Selection](#feature)\n* [Final Processing](#final)\n* [Modelling](#model)\n* [Model Tuning - Hyperparameter GridSearch](#tuning)\n* [Model Performance](#performance)\n* [To Do in future versions!](#future)","metadata":{"tags":[]}},{"cell_type":"markdown","source":"# Introduction <a id=\"introduction\"></a>\n\nAnalysis, Feature Engineering and Modelling of the titanic dataset from [Kaggle](https://www.kaggle.com/competitions/titanic/overview).\n\nIn this notebook is my first attempt of a thorough analysis of the titanic dataset. The goal was to predict survivors of the tragic sinking of the titanic based passenger information such as age, sex and socio-economic status.\n\nI tried several models, both with and without tuning to both improve my result and learn along the way.\n\n**Best performing model: 83.4%**\n\n**Hope you enjoy, let me know how I can improve, and if you liked it, an upvote would help me out alot!**","metadata":{"tags":[]}},{"cell_type":"markdown","source":"## Columns in the dataset\n\nThe columns present in the dataset are as follows: \n1. **PassengerId**: This column assigns a unique identifier for each passenger.\n2. **Survived**: Specifies whether the given passenger survived or not (1 - survived, 0 - didn't survive)\n3. **Pclass**: The passenger's class. (1 = Upper Deck, 2 = Middle Deck, 3 = Lower Deck)\n4. **Name**: The name of the passenger. \n5. **Sex**: The sex of the passenger (male, female)\n6. **Age**: The age of the passenger in years. If the age is estimated, is it in the form of xx.5. \n7. **SibSp**: How many siblings or spouses the passenger had on board with them. Sibling = brother, sister, stepbrother, stepsister and Spouse = husband, wife (mistresses and fianc√©s were ignored)\n8. **Parch**: How many parents or children the passenger had on boad with them. Parent = mother and father, child = daughter, son, stepdaughter and stepson and some children travelled only with a nanny, therefore parch=0 for them.\n9. **Ticket**: The ticket of the passenger. \n10. **Fare**: The fare amount paid by the passenger for the trip. \n11. **Cabin**: The cabin in which the passenger stayed. \n12. **Embarked**: The place from which the passenger embarked (S, C, Q)","metadata":{}},{"cell_type":"markdown","source":"# House Keeping <a id=\"house\"></a>\n\n## Import Libraries, load dataset and do a short summary","metadata":{"tags":[]}},{"cell_type":"code","source":"# import libraries\nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_palette(\"YlGnBu\")\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\n\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV \n\nfrom sklearn.model_selection import cross_val_score\n#from sklearn.metrics import classification_report\n\n# load datasets\ndf_train = pd.read_csv('/kaggle/input/titanic/train.csv')\ndf_test = pd.read_csv('/kaggle/input/titanic/test.csv')\ndf_gender_submission = pd.read_csv('/kaggle/input/titanic/gender_submission.csv')\n\n# mark train and test sets for future split\ndf_train['train_test'] = 1\ndf_test['train_test'] = 0\ndf_test['Survived'] = np.NaN\n\n#combine to a single dataframe with all data for feature engineering\ndf_all = pd.concat((df_train, df_test))\n\n# print dataset shape and columns\nprint(f'''\nTrain Dataset:\nLoaded train dataset with shape {df_train.shape} ({df_train.shape[0]} rows and {df_train.shape[1]} columns)\n\nTest Dataset:\nLoaded test dataset with shape {df_test.shape} ({df_test.shape[0]} rows and {df_test.shape[1]} columns)\n\nSample Submission Dataset:\nLoaded sample submission dataset with shape {df_gender_submission.shape} ({df_gender_submission.shape[0]} rows and {df_gender_submission.shape[1]} columns)\n''')","metadata":{"execution":{"iopub.status.busy":"2023-01-11T13:27:22.498670Z","iopub.execute_input":"2023-01-11T13:27:22.499504Z","iopub.status.idle":"2023-01-11T13:27:24.298452Z","shell.execute_reply.started":"2023-01-11T13:27:22.499401Z","shell.execute_reply":"2023-01-11T13:27:24.296956Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train dataset","metadata":{}},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-11T13:27:24.359741Z","iopub.execute_input":"2023-01-11T13:27:24.360102Z","iopub.status.idle":"2023-01-11T13:27:24.387493Z","shell.execute_reply.started":"2023-01-11T13:27:24.360070Z","shell.execute_reply":"2023-01-11T13:27:24.386524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.info()","metadata":{"execution":{"iopub.status.busy":"2023-01-11T13:27:24.389911Z","iopub.execute_input":"2023-01-11T13:27:24.390607Z","iopub.status.idle":"2023-01-11T13:27:24.412614Z","shell.execute_reply.started":"2023-01-11T13:27:24.390570Z","shell.execute_reply":"2023-01-11T13:27:24.411567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.describe()","metadata":{"execution":{"iopub.status.busy":"2023-01-11T13:27:24.414360Z","iopub.execute_input":"2023-01-11T13:27:24.414765Z","iopub.status.idle":"2023-01-11T13:27:24.455089Z","shell.execute_reply.started":"2023-01-11T13:27:24.414730Z","shell.execute_reply":"2023-01-11T13:27:24.453763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Initial thoughs\n\n* The **PassengerID** column shouldnt provide any useful information about survival, so it should be dropped.\n* The **Fare** column looks very volatile on the high end, Q3 (75%) = 31 and MAX = 512, maybe outliers?\n* There are null values in **Age**, **Cabin** and **Embarked**, these should be fixed, maybe **Age** and **Embarked** are missing data and **Cabin** is simply due to not every passenger having a **Cabin**\n* Both numerical and categorical columns. They should be examined further and either scaled or one hot encoded to improve model performance","metadata":{}},{"cell_type":"markdown","source":"# Exploratory Data Analysis <a id=\"EDA\"></a>\n\n## Survival ratio","metadata":{"jp-MarkdownHeadingCollapsed":true,"tags":[]}},{"cell_type":"code","source":"df_survived = df_train['Survived']\n\nsurvival, mortality = df_survived.value_counts() \n\nprint(f'''\nThere were {survival} survivors and {mortality} mortalities in the train set.\nMaking the survival rate {df_survived.mean():.2%}\n''')\n\nsns.countplot(x = df_survived)\nplt.title('Distribution of survival or mortality')","metadata":{"execution":{"iopub.status.busy":"2023-01-11T13:27:24.457198Z","iopub.execute_input":"2023-01-11T13:27:24.458157Z","iopub.status.idle":"2023-01-11T13:27:24.669780Z","shell.execute_reply.started":"2023-01-11T13:27:24.458115Z","shell.execute_reply":"2023-01-11T13:27:24.668859Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Name","metadata":{}},{"cell_type":"code","source":"# Extract titles\ndf_train['Title'] = (df_train['Name'].str.split(',', expand=True)[1]\n                                    .str.split('.', expand=True)[0])\n\n# List most frequent titles\n(df_train['Title'].value_counts()\n                  .to_frame()\n                  .reset_index()\n                  .iloc[:6]\n                  .rename(columns={'index':'Title', 'Title':'Frequency'}))","metadata":{"execution":{"iopub.status.busy":"2023-01-11T13:27:38.292329Z","iopub.execute_input":"2023-01-11T13:27:38.292775Z","iopub.status.idle":"2023-01-11T13:27:38.313424Z","shell.execute_reply.started":"2023-01-11T13:27:38.292738Z","shell.execute_reply":"2023-01-11T13:27:38.312505Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ticket Class","metadata":{}},{"cell_type":"code","source":"df_pclass = df_train['Pclass']\n\nupper, middle, lower = df_pclass.value_counts()\n\nprint(f'''\nPassengers were split into three Ticket Classes and hereby the placement on the ship deck:\nThere were {upper} people on the upper deck.\nThere were {middle} people on the middle deck.\nThere were {lower} people on the lower deck\n''')\n\nsns.countplot(x = df_pclass)\nplt.title('Distribution of ticket classes')","metadata":{"execution":{"iopub.status.busy":"2023-01-11T13:27:42.233443Z","iopub.execute_input":"2023-01-11T13:27:42.233929Z","iopub.status.idle":"2023-01-11T13:27:42.382447Z","shell.execute_reply.started":"2023-01-11T13:27:42.233887Z","shell.execute_reply":"2023-01-11T13:27:42.381134Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Passenger sex","metadata":{}},{"cell_type":"code","source":"df_sex = df_train['Sex']\n\nmale, female = df_sex.value_counts().sort_index()\n\nprint(f'''\nThere were {male} males aboard.\nThere were {female} females aboard.\n''') \n\nsns.countplot(x = df_sex)\nplt.title('Distribution of passenger sex')","metadata":{"execution":{"iopub.status.busy":"2023-01-11T13:27:43.632460Z","iopub.execute_input":"2023-01-11T13:27:43.632928Z","iopub.status.idle":"2023-01-11T13:27:43.786744Z","shell.execute_reply.started":"2023-01-11T13:27:43.632891Z","shell.execute_reply":"2023-01-11T13:27:43.784584Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Passenger age","metadata":{}},{"cell_type":"code","source":"df_age = df_train['Age']\n\nprint(f'''\nThere were {np.count_nonzero(df_age < 25)} passenges under the age of 25.\nThere were {np.count_nonzero((df_age >= 25) & (df_age <= 65))} passengers between the age of 25 and 65.\nThere were {np.count_nonzero(df_age > 65)} passenges older than 65.\n''') \n\nsns.histplot(data = df_age)\nplt.title('Distribution of passenger age')","metadata":{"execution":{"iopub.status.busy":"2023-01-11T13:27:43.969350Z","iopub.execute_input":"2023-01-11T13:27:43.970940Z","iopub.status.idle":"2023-01-11T13:27:44.294203Z","shell.execute_reply.started":"2023-01-11T13:27:43.970869Z","shell.execute_reply":"2023-01-11T13:27:44.292382Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Number of siblings/spouses","metadata":{}},{"cell_type":"code","source":"df_sibsp = df_train['SibSp']\n\nprint(f'There were {df_sibsp.value_counts().sort_index()[0]} passengers with no siblings or spouses.')\n\nsns.countplot(x = df_sibsp)\nplt.title('Distribution of number of siblings/spouses aboard')","metadata":{"execution":{"iopub.status.busy":"2023-01-11T13:27:44.414138Z","iopub.execute_input":"2023-01-11T13:27:44.414624Z","iopub.status.idle":"2023-01-11T13:27:44.665785Z","shell.execute_reply.started":"2023-01-11T13:27:44.414586Z","shell.execute_reply":"2023-01-11T13:27:44.664558Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Number of parents/children","metadata":{}},{"cell_type":"code","source":"df_parch = df_train['Parch']\n\nprint(f'There were {df_parch.value_counts().sort_index()[0]} passengers with no parents or children.')\n\nsns.countplot(x = df_parch)\nplt.title('Distribution of number of parents/children aboard')","metadata":{"execution":{"iopub.status.busy":"2023-01-11T13:27:44.788876Z","iopub.execute_input":"2023-01-11T13:27:44.789265Z","iopub.status.idle":"2023-01-11T13:27:45.035069Z","shell.execute_reply.started":"2023-01-11T13:27:44.789234Z","shell.execute_reply":"2023-01-11T13:27:45.033754Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tickets","metadata":{}},{"cell_type":"code","source":"df_ticket = df_train['Ticket']\n\n\n\nprint(f'''\nThere were {np.count_nonzero(df_ticket.value_counts() == 1)} passengers who bought their ticket alone.\nThere were {np.count_nonzero(df_ticket.value_counts() > 1)} passengers who bought tickets together.\n''') \n\nsns.histplot(data = df_ticket.value_counts())\nplt.title('Distribution of people per ticket')","metadata":{"execution":{"iopub.status.busy":"2023-01-11T13:27:45.165209Z","iopub.execute_input":"2023-01-11T13:27:45.165636Z","iopub.status.idle":"2023-01-11T13:27:45.433085Z","shell.execute_reply.started":"2023-01-11T13:27:45.165602Z","shell.execute_reply":"2023-01-11T13:27:45.432181Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fare","metadata":{}},{"cell_type":"code","source":"df_fare = df_train['Fare']\n\nprint(f'''\nThere were {np.count_nonzero(df_fare < 10)} passengers payed less than 10 dollars for their ticket.\nThere were {np.count_nonzero((df_fare >= 10) & (df_fare <= 50))} passengers payed between 10 and 50 dollars for their ticket.\nThere were {np.count_nonzero(df_fare > 50)} passengers payed more than 50 dollars for their ticket.\n''') \n\n\nsns.histplot(data = df_fare)\nplt.title('Distribution of fares')","metadata":{"execution":{"iopub.status.busy":"2023-01-11T13:27:45.535672Z","iopub.execute_input":"2023-01-11T13:27:45.536271Z","iopub.status.idle":"2023-01-11T13:27:45.969801Z","shell.execute_reply.started":"2023-01-11T13:27:45.536238Z","shell.execute_reply":"2023-01-11T13:27:45.968588Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cabin","metadata":{}},{"cell_type":"code","source":"df_cabin = df_train['Cabin']\n\nprint(f'''\nThere were {df_cabin.notna().astype(int).sum()} passengers who had a cabin. \nThere were {df_cabin.isna().astype(int).sum()} passengers who did not have a cabin.\n''') \n\ndf_cabin = np.where(df_cabin.isna(), 0, 1)\n\nsns.countplot(x = df_cabin)\nplt.title('Distribution of number of passengers with a cabin')","metadata":{"execution":{"iopub.status.busy":"2023-01-11T13:27:45.971598Z","iopub.execute_input":"2023-01-11T13:27:45.972002Z","iopub.status.idle":"2023-01-11T13:27:46.184554Z","shell.execute_reply.started":"2023-01-11T13:27:45.971966Z","shell.execute_reply":"2023-01-11T13:27:46.183511Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Port of Embarkation","metadata":{}},{"cell_type":"code","source":"df_port = df_train['Embarked']\n\nC, Q, S = df_port.value_counts().sort_index()\n\nprint(f'''\nThere were {S} passengers boarding the ship at Southampton.\nThere were {C} passengers boarding the ship at Cherbourg.\nThere were {Q} passengers boarding the ship at Queenstown.\n''') \n\nsns.countplot(x = df_port)\nplt.title('Distribution of number of passengers with a cabin')","metadata":{"execution":{"iopub.status.busy":"2023-01-11T13:27:46.277624Z","iopub.execute_input":"2023-01-11T13:27:46.278075Z","iopub.status.idle":"2023-01-11T13:27:46.497450Z","shell.execute_reply.started":"2023-01-11T13:27:46.278039Z","shell.execute_reply":"2023-01-11T13:27:46.496675Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Survival rate factors","metadata":{}},{"cell_type":"code","source":"sns.catplot(data=df_train, \n            x=\"Sex\", \n            y=\"Survived\", \n            hue=\"Pclass\", \n            kind=\"bar\")\n\nplt.title('Survival rate based on sex and passanger class')","metadata":{"execution":{"iopub.status.busy":"2023-01-11T13:27:46.664207Z","iopub.execute_input":"2023-01-11T13:27:46.665469Z","iopub.status.idle":"2023-01-11T13:27:47.201368Z","shell.execute_reply.started":"2023-01-11T13:27:46.665426Z","shell.execute_reply":"2023-01-11T13:27:47.199936Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Selection <a id=\"feature\"></a>\n\nOne of the best ways of getting column correlations is a **confusion matrix**. A **confusion matrix** plots the correlation of every column compared to each other, returning a matrix of scores. A score between 0 and -1 indicates negative correlation and 0 and 1 indicates positive correlations with values closer to -1/1 indicating stronger correlations.\n\n\nA **heatmap** takes this a step further, adding a color scale to the values, making correlations easier to spot at a glance.","metadata":{"jp-MarkdownHeadingCollapsed":true,"tags":[]}},{"cell_type":"code","source":"# change cabin names and numbers to cabin yes or no\ndf_train['Cabin'] = np.where(df_train['Cabin'].isna(), 0, 1)\n\n# change male/female to 0 and 1\ndf_train['Sex'] = np.where(df_train['Sex'] == 'female', 1, 0)\n\n# One-Hot encode Embarkation (done with pd.get_dummies() further down)\ndf_train.loc[df_train['Embarked'] == 'S', 'embarked_Southampton'] = 1\ndf_train.loc[df_train['Embarked'] == 'C', 'embarked_Cherbough'] = 1\ndf_train.loc[df_train['Embarked'] == 'Q', 'embarked_Queenstown'] = 1\n\ndf_train = df_train.drop('Embarked', axis = 1)\n\ndf_train = df_train.replace(np.nan, 0)","metadata":{"execution":{"iopub.status.busy":"2023-01-11T13:27:47.204736Z","iopub.execute_input":"2023-01-11T13:27:47.205538Z","iopub.status.idle":"2023-01-11T13:27:47.222543Z","shell.execute_reply.started":"2023-01-11T13:27:47.205495Z","shell.execute_reply":"2023-01-11T13:27:47.221093Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Corelation matrix of numerical categories\n(df_train[[\n          'PassengerId', \n          'Survived', \n          'Age',\n          'SibSp',\n          'Parch',\n          'Fare',\n          'Cabin',\n          'Pclass',\n          'embarked_Southampton',\n          'embarked_Cherbough',\n          'embarked_Queenstown']]\n          .corr())","metadata":{"execution":{"iopub.status.busy":"2023-01-11T13:27:47.237090Z","iopub.execute_input":"2023-01-11T13:27:47.237478Z","iopub.status.idle":"2023-01-11T13:27:47.264522Z","shell.execute_reply.started":"2023-01-11T13:27:47.237447Z","shell.execute_reply":"2023-01-11T13:27:47.262778Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Heatmap of correlation matrix for training data columns\n\nfig, ax = plt.subplots(figsize=(12,8)) \n\nsns.heatmap((df_train[[\n                      'PassengerId', \n                      'Survived', \n                      'Age',\n                      'SibSp',\n                      'Parch',\n                      'Fare',\n                      'Cabin',\n                      'Pclass',\n                      'embarked_Southampton',\n                      'embarked_Cherbough',\n                      'embarked_Queenstown'\n                      ]]\n                      .corr()),\n                      linewidths=1,\n                      cmap=plt.cm.Blues, \n                      annot=True,\n                      ax=ax)\n\nplt.title('Heatmap for correlation between columns of training data')","metadata":{"execution":{"iopub.status.busy":"2023-01-11T13:27:47.440506Z","iopub.execute_input":"2023-01-11T13:27:47.440922Z","iopub.status.idle":"2023-01-11T13:27:48.635354Z","shell.execute_reply.started":"2023-01-11T13:27:47.440890Z","shell.execute_reply":"2023-01-11T13:27:48.634027Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Heatmap Conclusion\n\nThe **heatmap** above shows correlation between all our features. Especially interesting is the feature correlation with our label, ```Survived```. Remember that a postive number close to 1 indicates a strong, positive correlation, while a negative number close to -1 indicates a strong, negative correlation. Numbers closer to 0 indicates a weak correlation:\n\n* ```Fare``` and ```Cabin``` seems to have the strongest positive correlations with our label (**0.26** and **0.32** respectively)\n* ```Pclass``` seems to have a strong negative correlation with out label (**-0.34**). However, remember, the upper deck is encoded as 1, middel as 2 and lower as 3. This means that the negative correlation with ```Survived``` has to be put in that context: A higher value lowers survival rate, the lower you are on the ship, the lower your survival rate is, which is what we would expect.\n* Maybe a bit surprisingly, we see a positive correlation for ```embarked_Cherbough``` and a negative correlation for ```embarked_Southampton```. This is most likely due to where workers on the ship boarded the ship or the order, that the people who embarked last, were placed higher up in the ship, giving them a better survival rate.\n* ```SibSp```, ```Parch``` and ```Age``` all show a correlation, however I would, at first glance, assume that ```Age``` had larger influence over the survival rate, but the dataset doesn't support my hypothesis.\n\nBased on these correlations, we can make better decisions on which features to keep, engineer og drop from the dataset before we start modelling.","metadata":{}},{"cell_type":"markdown","source":"# Final Processing <a id=\"final\"></a>\n\nBefore we model our data, some processing are encouraged to increase model performance.\n\nThere are several ways to treat different data types, but the following are often a guide first try:\n\n* **Remove null values:** Most sk.learn models dont accept null values, so these have to be fixed.\n* **OneHotEncode categorical columns:** Encode categorical features to a more machine learning friendly format. Our ```Cabin``` feature should be transformed into ```is_A```, ```is_B``` and ```is_C``` columns with 1/0 depending on which cabin the passenger had.\n* **Scale numerical data:** Numerical values on different scales function poorly in a model. A scaler scales (duh) the data to values between -1 and 1. This keep the relative different of each feature, but allows much better model performance with several features.\n\nNext our features can drastically improve model performance. Feature engineering differs based on approach, dataset and model. For the Titanic dataset I have done the following but there are many more ways to improve my features:  \n\n* **Change Cabin names and numbers:** Change ```Cabin``` to 1 or 0 for either having or not having a cabin.\n* **Extract Titles:** The ```Name``` feature by itself provides little value in predicting wether a passanger survived or not. Maybe extracting the titles from ```Name``` provides better accuracy.","metadata":{"jp-MarkdownHeadingCollapsed":true,"tags":[]}},{"cell_type":"code","source":"# impute nulls for continuous data \ndf_all.Age = df_all.Age.fillna(df_train.Age.median())\ndf_all.Fare = df_all.Fare.fillna(df_train.Fare.median())\n\n# drop null the two null Embarked values\ndf_all.dropna(subset=['Embarked'], inplace = True)\n\n# change cabin names and numbers to cabin yes or no\ndf_all['Cabin'] = np.where(df_all['Cabin'].isna(), 0, 1)\n\n# extract titles\ndf_all['Title'] = (df_all['Name'].str.split(',', expand=True)[1]\n                                 .str.split('.', expand=True)[0])\n\n# drop unneeded columns\ndf_all = (df_all.drop([\n                      'PassengerId',\n                      'Name',\n                      'Title',\n                      'Ticket',\n                      'Name'],\n                      axis = 1\n                      ))\n\ndf_all['Pclass'] = df_all['Pclass'].astype(str)\n\n# make dummies (OneHotEncode) categorical variables\ndf_all_dummies = pd.get_dummies(df_all[['Pclass', \n                                        'Sex', \n                                        'Age',\n                                        'SibSp', \n                                        'Parch', \n                                        'Fare', \n                                        'Cabin', \n                                        'Embarked', \n                                        'train_test']])\n\n# Scale data\nscaler = StandardScaler()\n(df_all_dummies[['Age', \n                 'SibSp', \n                 'Parch', \n                 'Fare']]) = (scaler.fit_transform(df_all_dummies[['Age', \n                                                                   'SibSp', \n                                                                   'Parch', \n                                                                   'Fare']]))\n\n# resplit into train and test sets\nX_train = df_all_dummies[df_all_dummies.train_test == 1].drop(['train_test'], axis =1)\nX_test = df_all_dummies[df_all_dummies.train_test == 0].drop(['train_test'], axis =1)\ny_train = df_all[df_all['train_test'] == 1]['Survived']\ny_test = df_all[df_all['train_test'] == 0]['Survived']\n\nprint(f'Before training models our train set has {X_train.shape} rows and columns and our test set has {X_test.shape} rows and columns.')","metadata":{"execution":{"iopub.status.busy":"2023-01-11T13:27:48.657370Z","iopub.execute_input":"2023-01-11T13:27:48.657877Z","iopub.status.idle":"2023-01-11T13:27:48.708936Z","shell.execute_reply.started":"2023-01-11T13:27:48.657844Z","shell.execute_reply":"2023-01-11T13:27:48.707582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelling <a id=\"model\"></a>\n\nWe have a preprocessed and model ready dataset, now to chose the right model. We are trying to predict which passangers survived the titanic sinkage, so most of the models we are testing, are **classification** models.\n\nTo get an indication of the best model, I will try several, baseline, models without any tuning. After getting an indication of which model performs best on our dataset, I will use grid search to tune the model hyperparameters to further improve the accuracy.\n\n* [Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html) - Naive Bayes models uses Bayes Theorem that offers conditional probability of events taking place.\n* [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n* [Decision Tree Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) - Decision Trees create a series of decisions to classify data based on the rules learned from the dataset.\n* [KNeighbors Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) - Neightbor Classifiers groups data with other data near to it based in a specified k value.\n* [Random Forest Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) - Random Forest classifiers fits a number of decision treees on subsamples of the dataset to improve the accuracy and redude over-fitting.\n* [Support Vector Classifier (SVC)](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)\n* [XGBoost Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) - Bootsting Classifiers builds additive models to allow optimization of the downstream models based on loss functions.\n* [Voting Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html) - Voting Classifier trains different models using the chosen algorithms, returning the majority's vote as the result.","metadata":{"execution":{"iopub.status.busy":"2023-01-08T16:27:08.236683Z","iopub.execute_input":"2023-01-08T16:27:08.237173Z","iopub.status.idle":"2023-01-08T16:27:08.245358Z","shell.execute_reply.started":"2023-01-08T16:27:08.237138Z","shell.execute_reply":"2023-01-08T16:27:08.243473Z"}}},{"cell_type":"code","source":"gnb = GaussianNB()\ncv = cross_val_score(gnb, \n                     X_train, \n                     y_train, \n                     cv=5)\n\nprint(f'GaussianNB: \\n{cv}\\nAverage: {cv.mean()}\\n')\n\nlr = LogisticRegression(max_iter=2000)\ncv = cross_val_score(lr, \n                     X_train, \n                     y_train, \n                     cv=5)\n\nprint(f'LogisticRegression: \\n{cv}\\nAverage: {cv.mean()}\\n')\n\ndt = tree.DecisionTreeClassifier(random_state=42)\ncv = cross_val_score(dt, \n                     X_train, \n                     y_train, \n                     cv=5)\n\nprint(f'DecisionTreeClassifier: \\n{cv}\\nAverage: {cv.mean()}\\n')\n\nknn = KNeighborsClassifier()\ncv = cross_val_score(knn, \n                     X_train, \n                     y_train, \n                     cv=5)\n\nprint(f'KNeighborsClassifier: \\n{cv}\\nAverage: {cv.mean()}\\n')\n\nrf = RandomForestClassifier(random_state=42)\ncv = cross_val_score(rf, \n                     X_train, \n                     y_train, \n                     cv=5)\n\nprint(f'RandomForestClassifier: \\n{cv}\\nAverage: {cv.mean()}\\n')\n\nsvc = SVC(probability=True)\ncv = cross_val_score(svc, \n                     X_train, \n                     y_train, \n                     cv=5)\n\nprint(f'SVC: \\n{cv}\\nAverage: {cv.mean()}\\n')\n\nxgb = XGBClassifier(random_state = 42)\ncv = cross_val_score(xgb, \n                     X_train, \n                     y_train, \n                     cv=5)\n\nprint(f'XGBClassifier: \\n{cv}\\nAverage: {cv.mean()}\\n')\n\nvoting_clf = VotingClassifier(estimators=[\n                                          ('lr', lr), \n                                          ('knn', knn), \n                                          ('rf', rf), \n                                          ('gnb', gnb), \n                                          ('dt', dt), \n                                          ('svc', svc), \n                                          ('xgb', xgb)],\n                                          voting='soft'\n                                          )\ncv = cross_val_score(voting_clf, X_train, y_train, cv=5)\n\nprint(f'VotingClassifier: \\n{cv}\\nAverage: {cv.mean()}\\n')","metadata":{"execution":{"iopub.status.busy":"2023-01-11T13:27:24.715584Z","iopub.status.idle":"2023-01-11T13:27:24.716006Z","shell.execute_reply.started":"2023-01-11T13:27:24.715815Z","shell.execute_reply":"2023-01-11T13:27:24.715835Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Baseline submission of best performing model","metadata":{}},{"cell_type":"code","source":"voting_clf.fit(X_train, y_train)\n\ny_hat_baseline = voting_clf.predict(X_test).astype(int)\n\nbaseline_submission = pd.DataFrame({'PassengerId': df_test.PassengerId, \n                                    'Survived': y_hat_baseline})\n\nbaseline_submission.to_csv('baseline_submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-01-11T13:27:24.717771Z","iopub.status.idle":"2023-01-11T13:27:24.718265Z","shell.execute_reply.started":"2023-01-11T13:27:24.718071Z","shell.execute_reply":"2023-01-11T13:27:24.718090Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Baseline model performance\n\n|Model|Baseline Performance|\n|--|--|\n|Naive Bayes| 77.0%|\n|Logistic Regression| 80.5%| \n|Decision Tree Classifier| 78.4%|\n|KNN Classifier| 80.5%|\n|Random Forest Classifier| 80.5%|\n|**Support Vector Classifier**| **82.5%**|\n|Xtreme Gradient Boosting| 82.2%|\n|Voting Classifier| 82.0%|","metadata":{}},{"cell_type":"markdown","source":"The **Support Vector Classifier** performed best before any model tuning. Next up, hyperparameter tuning using GridSearch. GridSearch will search the ```parameter_grid``` for each possible combination of parameter to find the best performing model. It's like running the model alot of times, searching for the best parameter combination. Some models, like **Naive Bayes**, has very few hyperparameters to tune, so they wont be tuned. \n\nA note on the **Random Tree Classifier**: Since the ```parameter_grid``` is so large, a GridSearch looking for all possible combinations, would take way too long to train. One way of dealing with this issue, is it first run a RandomizedGridSearch, which looks at the same ```parameter_grid```, but instead of looking at all combinations, it will check random parameter combinations. This drastically reduces the training time, but may miss the best combination of parameters. The best parameter combination from the RandomSearchCV is then used to guide a more narrow GridSearch, to find the best combination. This two-grid search approach can prove more realistic when then hyperparameter space is very large, like in a **Random Tree** model.","metadata":{}},{"cell_type":"markdown","source":"# Model Tuning - Hyperparameter GridSearch <a id=\"tuning\"></a>","metadata":{"tags":[]}},{"cell_type":"code","source":"def model_performance(model, name):\n    print(name)\n    print(f'Best Score: {model.best_score_}')\n    print(f'Best Parameters: {model.best_params_}\\n')  \n\nlr = LogisticRegression()\n\nparameter_grid = {'max_iter' : [2000],\n                  'penalty' : ['l1', 'l2'],\n                  'C' : np.logspace(-4, 4, 20),\n                  'solver' : ['liblinear'\n                  ]}\n\nlr_model = GridSearchCV(lr, \n                        param_grid=parameter_grid, \n                        cv=5, \n                        verbose=True, \n                        n_jobs=1)\nbest_lr_model = lr_model.fit(X_train, y_train)\nmodel_performance(best_lr_model, 'LogisticRegression')\n\n\nknn = KNeighborsClassifier()\n\nparameter_grid = {'n_neighbors' : [3,5,7,9],\n                  'weights' : ['uniform', 'distance'],\n                  'algorithm' : ['auto', 'ball_tree','kd_tree'],\n                  'p' : [1,2]}\n\nknn_model = GridSearchCV(knn, \n                         param_grid=parameter_grid, \n                         cv=5, \n                         verbose=True, \n                         n_jobs=-1)\nbest_knn_model = knn_model.fit(X_train, y_train)\nmodel_performance(best_knn_model, 'KNN')\n\n\nsvc = SVC(probability=True)\n\nparameter_grid = {'kernel': ['rbf'], \n                   'gamma': [.1,.5,1],\n                   'C': [.1, 1, 10]}\n\nsvc_model = GridSearchCV(svc, \n                       param_grid=parameter_grid, \n                       cv=5, \n                       verbose=True, \n                       n_jobs=-1)\nbest_svc_model = svc_model.fit(X_train, y_train)\nmodel_performance(best_svc_model,'SVC')\n\n\nrf = RandomForestClassifier(random_state=42)\n\nparameter_grid = {'n_estimators': [100,500], \n                  'bootstrap': [True,False],\n                  'max_depth': [10,20,50,75,None],\n                  'max_features': ['auto','sqrt'],\n                  'min_samples_leaf': [1,2,4],\n                  'min_samples_split': [2,5,10]}\n                                  \nrf_model_randomcv = RandomizedSearchCV(rf, \n                                param_distributions=parameter_grid, \n                                n_iter=50, \n                                cv=5, \n                                verbose=True, \n                                n_jobs=-1)\nbest_rf_model_randomcv = rf_model_randomcv.fit(X_train, y_train)\nmodel_performance(best_rf_model_randomcv, 'Random Forest (RandSearchCV)')\n\n\nrf = RandomForestClassifier(random_state=42)\n\nparameter_grid = {'n_estimators': [500,550,600],\n              'criterion':['gini'],\n              'bootstrap': [True],\n              'max_depth': [10, 15, 20],\n              'max_features': ['auto','sqrt', 10],\n              'min_samples_leaf': [2,3],\n              'min_samples_split': [2,3]}\n                                  \nrf_model = GridSearchCV(rf, \n                      param_grid = parameter_grid, \n                      cv=5, \n                      verbose=True, \n                      n_jobs=-1)\nbest_rf_model = rf_model.fit(X_train, y_train)\nmodel_performance(best_rf_model, 'Random Forest (GridSearchCV)')\n\n\nbest_rf_model.fit(X_train, y_train)\n\ny_hat_tuned = best_rf_model.predict(X_test).astype(int)\n\ntuned_submission = pd.DataFrame({'PassengerId': df_test.PassengerId, \n                                 'Survived': y_hat_tuned})\n\ntuned_submission.to_csv('tuned_submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-01-11T13:27:24.720035Z","iopub.status.idle":"2023-01-11T13:27:24.720481Z","shell.execute_reply.started":"2023-01-11T13:27:24.720276Z","shell.execute_reply":"2023-01-11T13:27:24.720296Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model performance <a id=\"performance\"></a>","metadata":{"tags":[]}},{"cell_type":"markdown","source":"|Model|Scaled Performance|Scaled and Tuned Performance|\n|--|--|--|\n|Naive Bayes| 77.0%| NA|\n|Logistic Regression| 80.5%| 80.7%|\n|Decision Tree Classifier| 78.4%| NA|\n|KNN Classifier| 80.5%|80.7%|\n|**Random Forest Classifier**| 80.5%| **83.2%**|\n|Support Vector Classifier| **82.5%**| 82.9%|\n|Xtreme Gradient Boosting| 82.2%| 81.0%|\n|Voting Classifier| 82.0%| NA|","metadata":{}},{"cell_type":"markdown","source":"Wow! Tuning the hyperparameters really improved the model performance, especially for the **Random Forest Classifier.**\n\nThere is plenty more to try with the modelling, but we will accept this for now.\n\n**Feel free to try other models and let me know!**","metadata":{}},{"cell_type":"markdown","source":"## Submissions","metadata":{}},{"cell_type":"markdown","source":"Submission of the baseline and tuned model. Both submission .csv files are made to match the sample submission file with ```PassengerID``` and ```Survived``` columns.","metadata":{}},{"cell_type":"code","source":"baseline_submission","metadata":{"execution":{"iopub.status.busy":"2023-01-11T13:27:24.721980Z","iopub.status.idle":"2023-01-11T13:27:24.722674Z","shell.execute_reply.started":"2023-01-11T13:27:24.722431Z","shell.execute_reply":"2023-01-11T13:27:24.722464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tuned_submission","metadata":{"execution":{"iopub.status.busy":"2023-01-11T13:27:24.724026Z","iopub.status.idle":"2023-01-11T13:27:24.724768Z","shell.execute_reply.started":"2023-01-11T13:27:24.724531Z","shell.execute_reply":"2023-01-11T13:27:24.724551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# To do in future versions! <a id=\"future\"></a>\n\n* Exploratory Data Analysis:\n    * Perform further EDA\n* Feature Engineering:\n    * Group and bin age and fare features\n    * Fare/Ticket# Column\n    * Remove fare outliers\n    * SMOTE Survival rates?\n* Model Evaluation:\n    * Make classification_report and ROC-curve","metadata":{}},{"cell_type":"code","source":"#%%capture --no-display\n#xgb = XGBClassifier(random_state=42)\n\n#parametere_grid = {\n              #'n_estimators': [450,500,550],\n              #'colsample_bytree': [0.75,0.8,0.85],\n              #'max_depth': [None],\n              #'reg_alpha': [1],\n              #'reg_lambda': [2, 5, 10],\n              #'subsample': [0.55, 0.6, .65],\n              #'learning_rate':[0.5],\n              #'gamma':[.5,1,2],\n              #'min_child_weight':[0.01],\n              #'sampling_method': ['uniform']}\n\n#xgb_model = GridSearchCV(xgb, \n                         #param_grid=parameter_grid, \n                         #cv=5, \n                         #verbose=True, \n                         #n_jobs=-1)\n\n#best_xgb_model = xgb_model.fit(X_train, y_train)\n\n#model_performance(best_xgb_model, 'XGB')","metadata":{"execution":{"iopub.status.busy":"2023-01-11T13:27:24.726031Z","iopub.status.idle":"2023-01-11T13:27:24.726942Z","shell.execute_reply.started":"2023-01-11T13:27:24.726725Z","shell.execute_reply":"2023-01-11T13:27:24.726746Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]}]}