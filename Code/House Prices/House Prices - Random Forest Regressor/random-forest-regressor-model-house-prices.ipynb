{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8018c46e",
   "metadata": {
    "papermill": {
     "duration": 0.006678,
     "end_time": "2023-01-24T11:17:53.937593",
     "exception": false,
     "start_time": "2023-01-24T11:17:53.930915",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Modelling - House Prices - Random Forest Approach\n",
    "\n",
    "![](https://raw.githubusercontent.com/UlrikThygePedersen/Kaggle/main/Code/House%20Prices%20-%20Advanced%20Regression%20Techniques/random_forest_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815f6169",
   "metadata": {
    "papermill": {
     "duration": 0.004677,
     "end_time": "2023-01-24T11:17:53.947774",
     "exception": false,
     "start_time": "2023-01-24T11:17:53.943097",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Table of Contents\n",
    "\n",
    "* [Introduction](#introduction)\n",
    "* [House Keeping](#house)\n",
    "* [Data Cleaning](#clean)\n",
    "* [Train-Test Split](#split)\n",
    "* [Random Forest Regressor Model](#forest)\n",
    "* [XGBoost Regressor Model](#XGB)\n",
    "* [Hyperparameter Tuning](#hyper)\n",
    "* [Conclusion](#conc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b80e69",
   "metadata": {
    "papermill": {
     "duration": 0.004624,
     "end_time": "2023-01-24T11:17:53.957364",
     "exception": false,
     "start_time": "2023-01-24T11:17:53.952740",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introduction <a id=\"introduction\"></a>\n",
    "\n",
    "*Machine learning modeling refers to the process of building and training a machine learning model to make predictions or decisions based on input data. The process typically involves the following steps:*\n",
    "\n",
    "* *Data preparation: Collecting and cleaning the data that will be used to train the model.*\n",
    "* *Feature engineering: Selecting and transforming the features that will be used by the model.*\n",
    "* *Model selection: Choosing an appropriate machine learning algorithm or model architecture that is suited to the problem at hand.*\n",
    "* *Model training: Using the prepared data to train the model.*\n",
    "* *Model evaluation: Evaluating the performance of the model using metrics such as accuracy, precision, recall, or F1-score.*\n",
    "* *Model tuning or optimization: Adjusting the hyperparameters of the model to improve its performance.*\n",
    "* *Model deployment: Putting the trained model into production, where it can be used to make predictions on new data.*\n",
    "\n",
    "*The goal of machine learning modeling is to build a model that can generalize well to new data and make accurate predictions or decisions. This process is iterative and may involve several rounds of model selection, training, and evaluation to find the best performing model.*\n",
    "\n",
    "In this notebook I will dive into the [House Prices - Advanced Regression Techniques](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques) dataset to explore and learn along the way.\n",
    "\n",
    "**Hope you enjoy, let me know how I can improve, and if you liked it, an upvote would help me out alot!**\n",
    "\n",
    "**Looking for Exploratory Data Analysis on this dataset? Check out my notebook [Exploratory Data Analysis](https://www.kaggle.com/code/ulrikthygepedersen/exploratory-data-analysis-house-prices/notebook)**\n",
    "\n",
    "**Want to learn more about making your data ready for modelling? Check out my notebook on [Feature Engineering](https://www.kaggle.com/code/ulrikthygepedersen/feature-engineering-house-prices/notebook)**\n",
    "\n",
    "**Want to learn more about how to further reduce features? Check out my notebook on [Principal Component Analysis](https://www.kaggle.com/code/ulrikthygepedersen/reducing-features-principal-component-analysis/notebook)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c43d5fa",
   "metadata": {
    "papermill": {
     "duration": 0.004777,
     "end_time": "2023-01-24T11:17:53.967609",
     "exception": false,
     "start_time": "2023-01-24T11:17:53.962832",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# House Keeping <a id=\"house\"></a>\n",
    "\n",
    "## Import Libraries, load dataset and do a short summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c27bf25",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-01-24T11:17:53.980512Z",
     "iopub.status.busy": "2023-01-24T11:17:53.979607Z",
     "iopub.status.idle": "2023-01-24T11:17:55.527614Z",
     "shell.execute_reply": "2023-01-24T11:17:55.526335Z"
    },
    "papermill": {
     "duration": 1.558041,
     "end_time": "2023-01-24T11:17:55.530608",
     "exception": false,
     "start_time": "2023-01-24T11:17:53.972567",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Dataset:\n",
      "Loaded train dataset with shape (1460, 82) (1460 rows and 82 columns)\n",
      "\n",
      "Test Dataset:\n",
      "Loaded test dataset with shape (1459, 81) (1459 rows and 81 columns)\n",
      "\n",
      "Sample Submission Dataset:\n",
      "Loaded sample submission dataset with shape (1459, 2) (1459 rows and 2 columns)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, cross_val_score\n",
    "\n",
    "# load datasets\n",
    "df_train = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\n",
    "df_test = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')\n",
    "df_sample_submission = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/sample_submission.csv')\n",
    "\n",
    "# mark train and test sets for future split\n",
    "df_train['train_test'] = 'Train'\n",
    "df_test['train_test'] = 'Test'\n",
    "\n",
    "#combine to a single dataframe with all data for feature engineering\n",
    "df_all = pd.concat([df_train, df_test])\n",
    "\n",
    "# print dataset shape and columns\n",
    "trow, tcol = df_train.shape\n",
    "erow, ecol = df_test.shape\n",
    "srow, scol = df_sample_submission.shape\n",
    "\n",
    "print(f'''\n",
    "Train Dataset:\n",
    "Loaded train dataset with shape {df_train.shape} ({trow} rows and {tcol} columns)\n",
    "\n",
    "Test Dataset:\n",
    "Loaded test dataset with shape {df_test.shape} ({erow} rows and {ecol} columns)\n",
    "\n",
    "Sample Submission Dataset:\n",
    "Loaded sample submission dataset with shape {df_sample_submission.shape} ({srow} rows and {scol} columns)\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4c6472",
   "metadata": {
    "papermill": {
     "duration": 0.00501,
     "end_time": "2023-01-24T11:17:55.540968",
     "exception": false,
     "start_time": "2023-01-24T11:17:55.535958",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Cleaning <a id=\"clean\"></a>\n",
    "\n",
    "Based on my [previous notebook on Exploratory Data Analysis](https://www.kaggle.com/code/ulrikthygepedersen/exploratory-data-analysis-house-prices), I will drop features with little information to increase model training time and accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dfc0e59",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-01-24T11:17:55.552533Z",
     "iopub.status.busy": "2023-01-24T11:17:55.552160Z",
     "iopub.status.idle": "2023-01-24T11:17:55.572683Z",
     "shell.execute_reply": "2023-01-24T11:17:55.571383Z"
    },
    "papermill": {
     "duration": 0.029341,
     "end_time": "2023-01-24T11:17:55.575367",
     "exception": false,
     "start_time": "2023-01-24T11:17:55.546026",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# drop the Id and PoolQC columns\n",
    "df_all = df_all.drop(['Id', \n",
    "                      'PoolQC', \n",
    "                      'PoolArea'], \n",
    "                      axis=1)\n",
    "\n",
    "# drop features with little information based on visualizations\n",
    "df_all = (df_all.drop(['BsmtFinSF2',\n",
    "                       'LowQualFinSF',\n",
    "                       'BsmtHalfBath',\n",
    "                       'KitchenAbvGr',\n",
    "                       'EnclosedPorch',\n",
    "                       '3SsnPorch',\n",
    "                       'MiscVal',\n",
    "                       'Street', \n",
    "                       'Utilities', \n",
    "                       'Condition2', \n",
    "                       'RoofMatl', \n",
    "                       'Heating',\n",
    "                       'MiscFeature'], \n",
    "                       axis=1))\n",
    "\n",
    "# drop features with little information based on heatmap\n",
    "df_all = (df_all.drop(['MSSubClass',\n",
    "                       'OverallCond',\n",
    "                       'ScreenPorch',\n",
    "                       'MoSold',\n",
    "                       'YrSold'], \n",
    "                       axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e229f18a",
   "metadata": {
    "papermill": {
     "duration": 0.004804,
     "end_time": "2023-01-24T11:17:55.585473",
     "exception": false,
     "start_time": "2023-01-24T11:17:55.580669",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature Engineering <a id=\"feature\"></a>\n",
    "\n",
    "Based on my [previous notebook on Feature Engineering](https://www.kaggle.com/code/ulrikthygepedersen/feature-engineering-house-prices), I will impute missing values, encode categorical features and scale numerical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81d453ac",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-01-24T11:17:55.597978Z",
     "iopub.status.busy": "2023-01-24T11:17:55.597562Z",
     "iopub.status.idle": "2023-01-24T11:17:55.780153Z",
     "shell.execute_reply": "2023-01-24T11:17:55.779197Z"
    },
    "papermill": {
     "duration": 0.19224,
     "end_time": "2023-01-24T11:17:55.782803",
     "exception": false,
     "start_time": "2023-01-24T11:17:55.590563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# replace numerical features with the mean of the column\n",
    "for col in df_all.columns:\n",
    "    if((df_all[col].dtype == 'float64') or (df_all[col].dtype == 'int64')):\n",
    "        df_all[col].fillna(df_all[col].mean(), inplace=True)\n",
    "\n",
    "# replace categorical features with the most common value of the column\n",
    "for col in df_all.columns:\n",
    "    if df_all[col].dtype == 'object':\n",
    "        df_all[col].fillna(df_all[col].mode()[0], inplace=True)\n",
    "        \n",
    "# encode ordinal features\n",
    "for col in ['BsmtQual', 'BsmtCond']:\n",
    "    OE = OrdinalEncoder(categories=[['No', 'Po', 'Fa', 'TA', 'Gd', 'Ex']])\n",
    "    df_all[col] = OE.fit_transform(df_all[[col]])\n",
    "\n",
    "    \n",
    "for col in ['ExterQual', 'ExterCond', 'KitchenQual']:\n",
    "    OE = OrdinalEncoder(categories=[['Po', 'Fa', 'TA', 'Gd', 'Ex']])\n",
    "    df_all[col] = OE.fit_transform(df_all[[col]])\n",
    "    \n",
    "\n",
    "OE = OrdinalEncoder(categories=[['N', 'P', 'Y']])\n",
    "df_all['PavedDrive'] = OE.fit_transform(df_all[['PavedDrive']])\n",
    "\n",
    "\n",
    "OE = OrdinalEncoder(categories=[['Mix', 'FuseP', 'FuseF', 'FuseA', 'SBrkr']])\n",
    "df_all['Electrical'] = OE.fit_transform(df_all[['Electrical']])\n",
    "\n",
    "\n",
    "for col in ['BsmtFinType1', 'BsmtFinType2']:\n",
    "    OE = OrdinalEncoder(categories=[['No', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ']])\n",
    "    df_all[col] = OE.fit_transform(df_all[[col]])\n",
    "\n",
    "\n",
    "OE = OrdinalEncoder(categories=[['C (all)', 'RH', 'RM', 'RL', 'FV']])\n",
    "df_all['MSZoning'] = OE.fit_transform(df_all[['MSZoning']])\n",
    "\n",
    "\n",
    "OE = OrdinalEncoder(categories=[['Slab', 'BrkTil', 'Stone', 'CBlock', 'Wood', 'PConc']])\n",
    "df_all['Foundation'] = OE.fit_transform(df_all[['Foundation']])\n",
    "\n",
    "\n",
    "OE = OrdinalEncoder(categories=[['MeadowV', 'IDOTRR', 'BrDale', 'Edwards', 'BrkSide', 'OldTown', 'NAmes', 'Sawyer', 'Mitchel', 'NPkVill', 'SWISU', 'Blueste', 'SawyerW', 'NWAmes', 'Gilbert', 'Blmngtn', 'ClearCr', 'Crawfor', 'CollgCr', 'Veenker', 'Timber', 'Somerst', 'NoRidge', 'StoneBr', 'NridgHt']])\n",
    "df_all['Neighborhood'] = OE.fit_transform(df_all[['Neighborhood']])\n",
    "\n",
    "\n",
    "OE = OrdinalEncoder(categories=[['None', 'BrkCmn', 'BrkFace', 'Stone']])\n",
    "df_all['MasVnrType'] = OE.fit_transform(df_all[['MasVnrType']])\n",
    "\n",
    "\n",
    "OE = OrdinalEncoder(categories=[['AdjLand', 'Abnorml','Alloca', 'Family', 'Normal', 'Partial']])\n",
    "df_all['SaleCondition'] = OE.fit_transform(df_all[['SaleCondition']])\n",
    "\n",
    "\n",
    "OE = OrdinalEncoder(categories=[['Gambrel', 'Gable','Hip', 'Mansard', 'Flat', 'Shed']])\n",
    "df_all['RoofStyle'] = OE.fit_transform(df_all[['RoofStyle']])\n",
    "\n",
    "# scale all numerical features\n",
    "numerical_features = df_all.select_dtypes(exclude=\"object\").columns\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "df_all[numerical_features] = scaler.fit_transform(df_all[numerical_features])\n",
    "\n",
    "# re add SalePrice\n",
    "df_train = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\n",
    "df_test = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')\n",
    "df_all2 = pd.concat([df_train, df_test])\n",
    "\n",
    "df_all['SalePrice'] = df_all2['SalePrice']\n",
    "\n",
    "# ONE HOT ENCODING - COMING SOON\n",
    "\n",
    "#one_hot_features = ['Alley', \n",
    "#                    'LotShape', \n",
    "#                    'LandContour', \n",
    "#                    'LotConfig', \n",
    "#                    'LandSlope', \n",
    "#                    'Condition1', \n",
    "#                    'GarageQual', \n",
    "#                    'GarageCond', \n",
    "#                    'Fence', \n",
    "#                    'SaleType']\n",
    "\n",
    "#df_dummies = pd.get_dummies(df_all[one_hot_features])\n",
    "\n",
    "#df_all = df_all.drop(one_hot_features, axis=1)\n",
    "\n",
    "#df_all = df_all.join(df_dummies)\n",
    "\n",
    "#df_all\n",
    "\n",
    "drop_col = ['Alley', \n",
    "            'LotShape', \n",
    "            'LandContour', \n",
    "            'LotConfig', \n",
    "            'LandSlope', \n",
    "            'Condition1', \n",
    "            'GarageQual', \n",
    "            'GarageCond', \n",
    "            'Fence', \n",
    "            'SaleType',\n",
    "           'BldgType',\n",
    "           'HouseStyle',\n",
    "           'Exterior1st',\n",
    "           'Exterior2nd',\n",
    "           'GarageFinish',\n",
    "           'GarageType',\n",
    "           'FireplaceQu',\n",
    "           'Functional',\n",
    "            'BsmtExposure',\n",
    "            'HeatingQC',\n",
    "            'CentralAir'\n",
    "           ]\n",
    "\n",
    "df_all = df_all.drop(drop_col, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51d06f3",
   "metadata": {
    "papermill": {
     "duration": 0.00479,
     "end_time": "2023-01-24T11:17:55.792965",
     "exception": false,
     "start_time": "2023-01-24T11:17:55.788175",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train-Test Split <a id=\"split\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ed9dda0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-24T11:17:55.804824Z",
     "iopub.status.busy": "2023-01-24T11:17:55.804439Z",
     "iopub.status.idle": "2023-01-24T11:17:55.819443Z",
     "shell.execute_reply": "2023-01-24T11:17:55.817960Z"
    },
    "papermill": {
     "duration": 0.024013,
     "end_time": "2023-01-24T11:17:55.822076",
     "exception": false,
     "start_time": "2023-01-24T11:17:55.798063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training models our train set has (1460, 38) rows and columns and our test set has (1459, 38) rows and columns.\n"
     ]
    }
   ],
   "source": [
    "# resplit into train and test sets\n",
    "X_train = df_all[df_all['train_test'] == 'Train'].drop(['train_test', 'SalePrice'], axis =1)\n",
    "X_test = df_all[df_all['train_test'] == 'Test'].drop(['train_test', 'SalePrice'], axis =1)\n",
    "y_train = df_all[df_all['train_test'] == 'Train']['SalePrice']\n",
    "y_test = df_all[df_all['train_test'] == 'Test']['SalePrice']\n",
    "\n",
    "print(f'Before training models our train set has {X_train.shape} rows and columns and our test set has {X_test.shape} rows and columns.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c3e75c",
   "metadata": {
    "papermill": {
     "duration": 0.004795,
     "end_time": "2023-01-24T11:17:55.832128",
     "exception": false,
     "start_time": "2023-01-24T11:17:55.827333",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Random Forest Regressor Model <a id=\"forest\"></a>\n",
    "\n",
    "A random forest regressor is a type of ensemble machine learning model that is used for regression tasks. It is built using a combination of multiple decision trees. Each decision tree is trained on a different subset of the data and with different subsets of the features. The final prediction is made by averaging the predictions of all the decision trees in the forest.\n",
    "\n",
    "The key idea behind a random forest regressor is to combine the predictions of multiple decision trees, which can decrease the variance and increase the stability of the model. Random forests are also less prone to overfitting than a single decision tree, as they average out the noise in the data.\n",
    "\n",
    "A random forest regressor is trained using a technique called bagging (Bootstrap Aggregating) which creates multiple training sets by randomly sampling the data with replacement. Each decision tree is then trained on a different bootstrapped training set, which leads to the creation of a diverse set of decision trees.\n",
    "\n",
    "Random forest regressor can be used for both linear and non-linear regression problem. It is a robust model, which works well for both high-dimensional and low-dimensional datasets and also works well for datasets with a large number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0616b246",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-24T11:17:55.845089Z",
     "iopub.status.busy": "2023-01-24T11:17:55.844372Z",
     "iopub.status.idle": "2023-01-24T11:17:57.452511Z",
     "shell.execute_reply": "2023-01-24T11:17:57.451355Z"
    },
    "papermill": {
     "duration": 1.61797,
     "end_time": "2023-01-24T11:17:57.455222",
     "exception": false,
     "start_time": "2023-01-24T11:17:55.837252",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define simple function to judge model performance\n",
    "def model_performance(model, name):\n",
    "    print(name)\n",
    "    print(f'Best Score: {model.best_score_}')\n",
    "    print(f'Best Parameters: {model.best_params_}\\n') \n",
    "\n",
    "# instanciate a Random Forest Regressor model\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# fit the model to the training data\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# use the model to predict on the test set\n",
    "y_hat = rf.predict(X_test).astype(int)\n",
    "\n",
    "# make a dataframe and save it as a csv for submission\n",
    "submission = pd.DataFrame({'Id': df_test['Id'], 'SalePrice': y_hat})\n",
    "submission.to_csv('simple_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5496b29",
   "metadata": {
    "papermill": {
     "duration": 0.005013,
     "end_time": "2023-01-24T11:17:57.465460",
     "exception": false,
     "start_time": "2023-01-24T11:17:57.460447",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Hyperparameter Tuning <a id=\"hyper\"></a>\n",
    "\n",
    "Hyperparameter tuning refers to the process of systematically searching for the best combination of hyperparameters for a machine learning model. Hyperparameters are parameters that are not learned from data, but set before the training process begins. \n",
    "\n",
    "Examples of hyperparameters include the learning rate of a neural network, the number of trees in a random forest, and the regularization term in a linear regression. Hyperparameter tuning is important because it can significantly impact the performance of a machine learning model. Common techniques for hyperparameter tuning include: \n",
    "\n",
    "* Grid search\n",
    "* Random search\n",
    "* Bayesian optimization\n",
    "\n",
    "Since our **Random Forest Regressor** has such a large **parameter space**, we are first using a **RandomizedSearchCV** to narrow it down and then a full **GridSearchCV** to fully optimize our model:\n",
    "\n",
    "## Randomized Search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b68379bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-24T11:17:57.477081Z",
     "iopub.status.busy": "2023-01-24T11:17:57.476662Z",
     "iopub.status.idle": "2023-01-24T11:18:46.892446Z",
     "shell.execute_reply": "2023-01-24T11:18:46.891050Z"
    },
    "papermill": {
     "duration": 49.427264,
     "end_time": "2023-01-24T11:18:46.897697",
     "exception": false,
     "start_time": "2023-01-24T11:17:57.470433",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Random Forest Regressor (RandomizedSearchCV)\n",
      "Best Score: 0.8887659710833334\n",
      "Best Parameters: {'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 50, 'bootstrap': False}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# instanciate a Random Forest Regressor model\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# set up a parameter grid to search for the best combination of hyperparameters\n",
    "parameter_grid = {'n_estimators': [50,100,200], \n",
    "                  'bootstrap': [True,False],\n",
    "                  'max_depth': [10,20,50,75,None],\n",
    "                  'max_features': ['auto','sqrt'],\n",
    "                  'min_samples_leaf': [1,2,4],\n",
    "                  'min_samples_split': [2,5,10]\n",
    "                  }\n",
    "\n",
    "# fit the model with all combinations of the parameters from the grid\n",
    "rf_random = RandomizedSearchCV(rf, \n",
    "                               param_distributions=parameter_grid,\n",
    "                               n_iter=50,\n",
    "                               cv=3, \n",
    "                               verbose=True,\n",
    "                               random_state=42,\n",
    "                               n_jobs=-1)\n",
    "\n",
    "# fit the model to the training data\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the model\n",
    "model_performance(rf_random, 'Random Forest Regressor (RandomizedSearchCV)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db15b79",
   "metadata": {
    "papermill": {
     "duration": 0.005002,
     "end_time": "2023-01-24T11:18:46.908805",
     "exception": false,
     "start_time": "2023-01-24T11:18:46.903803",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The most important arguments in RandomizedSearchCV are **n_iter**, which controls the number of different combinations to try, and **cv** which is the number of folds to use for cross validation. \n",
    "\n",
    "More **iterations** will cover a wider search space and more **cv** folds reduces the chances of overfitting, but raising each will increase the run time. Machine learning is a field of trade-offs, and **performance vs time** is one of the most fundamental.\n",
    "\n",
    "**We can view the best parameters from fitting the random search:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bbad525",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-24T11:18:46.921200Z",
     "iopub.status.busy": "2023-01-24T11:18:46.920743Z",
     "iopub.status.idle": "2023-01-24T11:18:46.932105Z",
     "shell.execute_reply": "2023-01-24T11:18:46.930663Z"
    },
    "papermill": {
     "duration": 0.020761,
     "end_time": "2023-01-24T11:18:46.934783",
     "exception": false,
     "start_time": "2023-01-24T11:18:46.914022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 200,\n",
       " 'min_samples_split': 5,\n",
       " 'min_samples_leaf': 1,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': 50,\n",
       " 'bootstrap': False}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcd88b2",
   "metadata": {
    "papermill": {
     "duration": 0.005958,
     "end_time": "2023-01-24T11:18:46.946209",
     "exception": false,
     "start_time": "2023-01-24T11:18:46.940251",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Random search allowed us to narrow down the range for each hyperparameter. Now that we know where to concentrate our search, we can explicitly specify every combination of settings to try. We do this with **GridSearchCV**, a method that, instead of sampling randomly from a distribution, evaluates all combinations we define. \n",
    "\n",
    "**To use Grid Search, we make another grid based on the best values provided by random search:**\n",
    "\n",
    "## Grid Search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9122a81b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-24T11:18:46.959507Z",
     "iopub.status.busy": "2023-01-24T11:18:46.958835Z",
     "iopub.status.idle": "2023-01-24T11:20:13.503749Z",
     "shell.execute_reply": "2023-01-24T11:20:13.502381Z"
    },
    "papermill": {
     "duration": 86.55886,
     "end_time": "2023-01-24T11:20:13.510366",
     "exception": false,
     "start_time": "2023-01-24T11:18:46.951506",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 54 candidates, totalling 162 fits\n",
      "Random Forest Regressor (GridSearchCV)\n",
      "Best Score: 0.8889382493998639\n",
      "Best Parameters: {'bootstrap': False, 'max_depth': 50, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 150}\n",
      "\n",
      "Fitting 3 folds for each of 54 candidates, totalling 162 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1461</td>\n",
       "      <td>123555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1462</td>\n",
       "      <td>154813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1463</td>\n",
       "      <td>181125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1464</td>\n",
       "      <td>187537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1465</td>\n",
       "      <td>189907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>2915</td>\n",
       "      <td>79406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>2916</td>\n",
       "      <td>84294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>2917</td>\n",
       "      <td>166861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>2918</td>\n",
       "      <td>114320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>2919</td>\n",
       "      <td>208048</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1459 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id  SalePrice\n",
       "0     1461     123555\n",
       "1     1462     154813\n",
       "2     1463     181125\n",
       "3     1464     187537\n",
       "4     1465     189907\n",
       "...    ...        ...\n",
       "1454  2915      79406\n",
       "1455  2916      84294\n",
       "1456  2917     166861\n",
       "1457  2918     114320\n",
       "1458  2919     208048\n",
       "\n",
       "[1459 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instanciate a Random Forest Regressor model\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# set up a parameter grid to search for the best combination of hyperparameters\n",
    "parameter_grid = {'n_estimators': [150,200,250], \n",
    "                  'bootstrap': [False],\n",
    "                  'max_depth': [25,50,75],\n",
    "                  'max_features': ['sqrt'],\n",
    "                  'min_samples_leaf': [1,2],\n",
    "                  'min_samples_split': [3,5,7],\n",
    "                  }\n",
    "\n",
    "# fit the model with all combinations of the parameters from the grid\n",
    "rf_grid = GridSearchCV(estimator = rf, \n",
    "                         param_grid=parameter_grid,\n",
    "                         cv=3, \n",
    "                         verbose=True,\n",
    "                         n_jobs=-1)\n",
    "\n",
    "# extract the best performing model and fit it to the data\n",
    "best_rf_model = rf_grid.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the model\n",
    "model_performance(best_rf_model, 'Random Forest Regressor (GridSearchCV)')\n",
    "\n",
    "best_rf_model.fit(X_train, y_train)\n",
    "\n",
    "# use the model to predict on the test set\n",
    "y_hat_tuned = best_rf_model.predict(X_test).astype(int)\n",
    "\n",
    "# make a dataframe and save it as a csv for submission\n",
    "tuned_submission = pd.DataFrame({'Id': df_test['Id'], 'SalePrice': y_hat_tuned})\n",
    "tuned_submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "# print the tuned submission dataframe\n",
    "tuned_submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452c983f",
   "metadata": {
    "papermill": {
     "duration": 0.005651,
     "end_time": "2023-01-24T11:20:13.521746",
     "exception": false,
     "start_time": "2023-01-24T11:20:13.516095",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Conclusion <a id=\"conc\"></a>\n",
    "\n",
    "And we have a submission!\n",
    "\n",
    "In this notebook we explored a **Random Forest Approach** to predicting the sale price of houses in the [House Prices - Advanced Regression Techniques dataset](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques)\n",
    "\n",
    "* First we prepared the data for modelling\n",
    "* Next we ran created a Random Forest model to predict Sale Price of our dataset\n",
    "* And last, but not least, we improved our model by tuning its hyperparameters\n",
    "\n",
    "**If you made it through all my notebooks, first of all, THANK YOU!**\n",
    "\n",
    "**It has been a blast exploring this dataset and learning along the way! Now to look for a new challenge, stay tuned and take care!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 150.658426,
   "end_time": "2023-01-24T11:20:16.149135",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-01-24T11:17:45.490709",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
