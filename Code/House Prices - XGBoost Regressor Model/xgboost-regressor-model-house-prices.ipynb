{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2a42b2e",
   "metadata": {
    "papermill": {
     "duration": 0.004462,
     "end_time": "2023-01-23T20:42:06.237097",
     "exception": false,
     "start_time": "2023-01-23T20:42:06.232635",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Modelling - House Prices\n",
    "\n",
    "![](https://mljar.com/images/machine-learning/random_forest_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb62550",
   "metadata": {
    "papermill": {
     "duration": 0.00325,
     "end_time": "2023-01-23T20:42:06.243953",
     "exception": false,
     "start_time": "2023-01-23T20:42:06.240703",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Table of Contents\n",
    "\n",
    "* [Introduction](#introduction)\n",
    "* [House Keeping](#house)\n",
    "* [Data Cleaning](#clean)\n",
    "* [Train-Test Split](#split)\n",
    "* [Random Forest Regressor Model](#forest)\n",
    "* [XGBoost Regressor Model](#XGB)\n",
    "* [Hyperparameter Tuning](#hyper)\n",
    "* [Conclusion](#conc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa35433",
   "metadata": {
    "papermill": {
     "duration": 0.003229,
     "end_time": "2023-01-23T20:42:06.250641",
     "exception": false,
     "start_time": "2023-01-23T20:42:06.247412",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introduction <a id=\"introduction\"></a>\n",
    "\n",
    "*Machine learning modeling refers to the process of building and training a machine learning model to make predictions or decisions based on input data. The process typically involves the following steps:*\n",
    "\n",
    "* *Data preparation: Collecting and cleaning the data that will be used to train the model.*\n",
    "* *Feature engineering: Selecting and transforming the features that will be used by the model.*\n",
    "* *Model selection: Choosing an appropriate machine learning algorithm or model architecture that is suited to the problem at hand.*\n",
    "* *Model training: Using the prepared data to train the model.*\n",
    "* *Model evaluation: Evaluating the performance of the model using metrics such as accuracy, precision, recall, or F1-score.*\n",
    "* *Model tuning or optimization: Adjusting the hyperparameters of the model to improve its performance.*\n",
    "* *Model deployment: Putting the trained model into production, where it can be used to make predictions on new data.*\n",
    "\n",
    "*The goal of machine learning modeling is to build a model that can generalize well to new data and make accurate predictions or decisions. This process is iterative and may involve several rounds of model selection, training, and evaluation to find the best performing model.*\n",
    "\n",
    "In this notebook I will dive into the [House Prices - Advanced Regression Techniques](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques) dataset to explore and learn along the way.\n",
    "\n",
    "**Hope you enjoy, let me know how I can improve, and if you liked it, an upvote would help me out alot!**\n",
    "\n",
    "**Looking for Exploratory Data Analysis on this dataset? Check out my notebook [Exploratory Data Analysis - House Prices](https://www.kaggle.com/code/ulrikthygepedersen/exploratory-data-analysis-house-prices/notebook)**\n",
    "\n",
    "**Want to learn more about making your data ready for modelling? Check out my notebook on [Feature Engineering](https://www.kaggle.com/code/ulrikthygepedersen/feature-engineering-house-prices/notebook)**\n",
    "\n",
    "**Want to learn more about how to further reduce features? Check out my notebook on [Principal Component Analysis](https://www.kaggle.com/code/ulrikthygepedersen/reducing-features-principal-component-analysis/notebook)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b34162",
   "metadata": {
    "papermill": {
     "duration": 0.003239,
     "end_time": "2023-01-23T20:42:06.257311",
     "exception": false,
     "start_time": "2023-01-23T20:42:06.254072",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# House Keeping <a id=\"house\"></a>\n",
    "\n",
    "## Import Libraries, load dataset and do a short summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2b94614",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-23T20:42:06.266460Z",
     "iopub.status.busy": "2023-01-23T20:42:06.266019Z",
     "iopub.status.idle": "2023-01-23T20:42:07.711785Z",
     "shell.execute_reply": "2023-01-23T20:42:07.709912Z"
    },
    "papermill": {
     "duration": 1.453537,
     "end_time": "2023-01-23T20:42:07.714223",
     "exception": false,
     "start_time": "2023-01-23T20:42:06.260686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Dataset:\n",
      "Loaded train dataset with shape (1460, 82) (1460 rows and 82 columns)\n",
      "\n",
      "Test Dataset:\n",
      "Loaded test dataset with shape (1459, 81) (1459 rows and 81 columns)\n",
      "\n",
      "Sample Submission Dataset:\n",
      "Loaded sample submission dataset with shape (1459, 2) (1459 rows and 2 columns)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# load datasets\n",
    "df_train = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\n",
    "df_test = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')\n",
    "df_sample_submission = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/sample_submission.csv')\n",
    "\n",
    "# mark train and test sets for future split\n",
    "df_train['train_test'] = 1\n",
    "df_test['train_test'] = 0\n",
    "\n",
    "#combine to a single dataframe with all data for feature engineering\n",
    "df_all = pd.concat([df_train, df_test])\n",
    "\n",
    "# print dataset shape and columns\n",
    "trow, tcol = df_train.shape\n",
    "erow, ecol = df_test.shape\n",
    "srow, scol = df_sample_submission.shape\n",
    "\n",
    "print(f'''\n",
    "Train Dataset:\n",
    "Loaded train dataset with shape {df_train.shape} ({trow} rows and {tcol} columns)\n",
    "\n",
    "Test Dataset:\n",
    "Loaded test dataset with shape {df_test.shape} ({erow} rows and {ecol} columns)\n",
    "\n",
    "Sample Submission Dataset:\n",
    "Loaded sample submission dataset with shape {df_sample_submission.shape} ({srow} rows and {scol} columns)\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ebb5f5",
   "metadata": {
    "papermill": {
     "duration": 0.003373,
     "end_time": "2023-01-23T20:42:07.721516",
     "exception": false,
     "start_time": "2023-01-23T20:42:07.718143",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Cleaning <a id=\"clean\"></a>\n",
    "\n",
    "Based on my [previous notebook on Exploratory Data Analysis](https://www.kaggle.com/code/ulrikthygepedersen/exploratory-data-analysis-house-prices), I will drop features with little information to increase model training time and accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c73923b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-23T20:42:07.730873Z",
     "iopub.status.busy": "2023-01-23T20:42:07.730455Z",
     "iopub.status.idle": "2023-01-23T20:42:07.748215Z",
     "shell.execute_reply": "2023-01-23T20:42:07.746982Z"
    },
    "papermill": {
     "duration": 0.025684,
     "end_time": "2023-01-23T20:42:07.751037",
     "exception": false,
     "start_time": "2023-01-23T20:42:07.725353",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# drop the Id and PoolQC columns\n",
    "df_all = df_all.drop(['Id', \n",
    "                      'PoolQC', \n",
    "                      'PoolArea'], \n",
    "                      axis=1)\n",
    "\n",
    "# drop features with little information based on visualizations\n",
    "df_all = (df_all.drop(['BsmtFinSF2',\n",
    "                       'LowQualFinSF',\n",
    "                       'BsmtHalfBath',\n",
    "                       'KitchenAbvGr',\n",
    "                       'EnclosedPorch',\n",
    "                       '3SsnPorch',\n",
    "                       'MiscVal',\n",
    "                       'Street', \n",
    "                       'Utilities', \n",
    "                       'Condition2', \n",
    "                       'RoofMatl', \n",
    "                       'Heating',\n",
    "                       'MiscFeature'], \n",
    "                       axis=1))\n",
    "\n",
    "# drop features with little information based on heatmap\n",
    "df_all = (df_all.drop(['MSSubClass',\n",
    "                       'OverallCond',\n",
    "                       'ScreenPorch',\n",
    "                       'MoSold',\n",
    "                       'YrSold'], \n",
    "                       axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8beb47",
   "metadata": {
    "papermill": {
     "duration": 0.004113,
     "end_time": "2023-01-23T20:42:07.759213",
     "exception": false,
     "start_time": "2023-01-23T20:42:07.755100",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature Engineering <a id=\"feature\"></a>\n",
    "\n",
    "Based on my [previous notebook on Feature Engineering](https://www.kaggle.com/code/ulrikthygepedersen/feature-engineering-house-prices), I will impute missing values, encode categorical features and scale numerical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f951f3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-23T20:42:07.769563Z",
     "iopub.status.busy": "2023-01-23T20:42:07.768520Z",
     "iopub.status.idle": "2023-01-23T20:42:07.907301Z",
     "shell.execute_reply": "2023-01-23T20:42:07.905268Z"
    },
    "papermill": {
     "duration": 0.147047,
     "end_time": "2023-01-23T20:42:07.909997",
     "exception": true,
     "start_time": "2023-01-23T20:42:07.762950",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OrdinalEncoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19/721453525.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# encode ordinal features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'BsmtQual'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'BsmtCond'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mOE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrdinalEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'No'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Po'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Fa'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'TA'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Gd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Ex'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mdf_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'OrdinalEncoder' is not defined"
     ]
    }
   ],
   "source": [
    "# replace numerical features with the mean of the column\n",
    "for col in df_all.columns:\n",
    "    if((df_all[col].dtype == 'float64') or (df_all[col].dtype == 'int64')):\n",
    "        df_all[col].fillna(df_all[col].mean(), inplace=True)\n",
    "\n",
    "# replace categorical features with the most common value of the column\n",
    "for col in df_all.columns:\n",
    "    if df_all[col].dtype == 'object':\n",
    "        df_all[col].fillna(df_all[col].mode()[0], inplace=True)\n",
    "        \n",
    "# encode ordinal features\n",
    "for col in ['BsmtQual', 'BsmtCond']:\n",
    "    OE = OrdinalEncoder(categories=[['No', 'Po', 'Fa', 'TA', 'Gd', 'Ex']])\n",
    "    df_all[col] = OE.fit_transform(df_all[[col]])\n",
    "\n",
    "    \n",
    "for col in ['ExterQual', 'ExterCond', 'KitchenQual']:\n",
    "    OE = OrdinalEncoder(categories=[['Po', 'Fa', 'TA', 'Gd', 'Ex']])\n",
    "    df_all[col] = OE.fit_transform(df_all[[col]])\n",
    "    \n",
    "\n",
    "OE = OrdinalEncoder(categories=[['N', 'P', 'Y']])\n",
    "df_all['PavedDrive'] = OE.fit_transform(df_all[['PavedDrive']])\n",
    "\n",
    "\n",
    "OE = OrdinalEncoder(categories=[['Mix', 'FuseP', 'FuseF', 'FuseA', 'SBrkr']])\n",
    "df_all['Electrical'] = OE.fit_transform(df_all[['Electrical']])\n",
    "\n",
    "\n",
    "for col in ['BsmtFinType1', 'BsmtFinType2']:\n",
    "    OE = OrdinalEncoder(categories=[['No', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ']])\n",
    "    df_all[col] = OE.fit_transform(df_all[[col]])\n",
    "\n",
    "\n",
    "OE = OrdinalEncoder(categories=[['C (all)', 'RH', 'RM', 'RL', 'FV']])\n",
    "df_all['MSZoning'] = OE.fit_transform(df_all[['MSZoning']])\n",
    "\n",
    "\n",
    "OE = OrdinalEncoder(categories=[['Slab', 'BrkTil', 'Stone', 'CBlock', 'Wood', 'PConc']])\n",
    "df_all['Foundation'] = OE.fit_transform(df_all[['Foundation']])\n",
    "\n",
    "\n",
    "OE = OrdinalEncoder(categories=[['MeadowV', 'IDOTRR', 'BrDale', 'Edwards', 'BrkSide', 'OldTown', 'NAmes', 'Sawyer', 'Mitchel', 'NPkVill', 'SWISU', 'Blueste', 'SawyerW', 'NWAmes', 'Gilbert', 'Blmngtn', 'ClearCr', 'Crawfor', 'CollgCr', 'Veenker', 'Timber', 'Somerst', 'NoRidge', 'StoneBr', 'NridgHt']])\n",
    "df_all['Neighborhood'] = OE.fit_transform(df_all[['Neighborhood']])\n",
    "\n",
    "\n",
    "OE = OrdinalEncoder(categories=[['None', 'BrkCmn', 'BrkFace', 'Stone']])\n",
    "df_all['MasVnrType'] = OE.fit_transform(df_all[['MasVnrType']])\n",
    "\n",
    "\n",
    "OE = OrdinalEncoder(categories=[['AdjLand', 'Abnorml','Alloca', 'Family', 'Normal', 'Partial']])\n",
    "df_all['SaleCondition'] = OE.fit_transform(df_all[['SaleCondition']])\n",
    "\n",
    "\n",
    "OE = OrdinalEncoder(categories=[['Gambrel', 'Gable','Hip', 'Mansard', 'Flat', 'Shed']])\n",
    "df_all['RoofStyle'] = OE.fit_transform(df_all[['RoofStyle']])\n",
    "\n",
    "# scale all numerical features\n",
    "numerical_features = df_all.select_dtypes(exclude=\"object\").columns\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "df_all[numerical_features] = scaler.fit_transform(df_all[numerical_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874eca52",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Train-Test Split <a id=\"split\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea12378",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "276ade47",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Random Forest Regressor Model <a id=\"forest\"></a>\n",
    "\n",
    "A random forest regressor is a type of ensemble machine learning model that is used for regression tasks. It is built using a combination of multiple decision trees. Each decision tree is trained on a different subset of the data and with different subsets of the features. The final prediction is made by averaging the predictions of all the decision trees in the forest.\n",
    "\n",
    "The key idea behind a random forest regressor is to combine the predictions of multiple decision trees, which can decrease the variance and increase the stability of the model. Random forests are also less prone to overfitting than a single decision tree, as they average out the noise in the data.\n",
    "\n",
    "A random forest regressor is trained using a technique called bagging (Bootstrap Aggregating) which creates multiple training sets by randomly sampling the data with replacement. Each decision tree is then trained on a different bootstrapped training set, which leads to the creation of a diverse set of decision trees.\n",
    "\n",
    "Random forest regressor can be used for both linear and non-linear regression problem. It is a robust model, which works well for both high-dimensional and low-dimensional datasets and also works well for datasets with a large number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ae3f54",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c856e6d3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# XGBoost Regressor Model <a id=\"XGB\"></a>\n",
    "\n",
    "XGBoost (eXtreme Gradient Boosting) is an open-source, distributed gradient boosting library that is used for supervised learning problems, such as regression and classification. XGBoost regressor is a specific implementation of the XGBoost library that is used for regression tasks.\n",
    "\n",
    "Like random forest regressor, XGBoost regressor is also an ensemble method, which combines the predictions of multiple decision trees. However, the main difference between XGBoost and random forest is the way they construct their decision trees. XGBoost uses a technique called gradient boosting, which builds decision trees in a sequential manner. Each tree is trained to correct the errors made by the previous tree.\n",
    "\n",
    "XGBoost also includes several other techniques to improve the performance of the model, such as regularization, which helps to prevent overfitting, and a technique called \"weighted quantile sketch\" which helps to handle missing values in the dataset.\n",
    "\n",
    "XGBoost regressor is a powerful model that is often used in machine learning competitions and has been known to perform well on a wide range of datasets. It is also computationally efficient and can scale well to large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552778c4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63728cb4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Hyperparameter Tuning <a id=\"hyper\"></a>\n",
    "\n",
    "Hyperparameter tuning refers to the process of systematically searching for the best combination of hyperparameters for a machine learning model. Hyperparameters are parameters that are not learned from data, but set before the training process begins. \n",
    "\n",
    "Examples of hyperparameters include the learning rate of a neural network, the number of trees in a random forest, and the regularization term in a linear regression. Hyperparameter tuning is important because it can significantly impact the performance of a machine learning model. Common techniques for hyperparameter tuning include: \n",
    "\n",
    "* Grid search\n",
    "* Random search\n",
    "* Bayesian optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bcbe8b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7177620",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Conclusion <a id=\"conc\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439be950",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10.119522,
   "end_time": "2023-01-23T20:42:08.636618",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-01-23T20:41:58.517096",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
