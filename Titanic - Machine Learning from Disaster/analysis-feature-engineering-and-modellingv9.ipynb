{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51d06866",
   "metadata": {
    "papermill": {
     "duration": 0.011715,
     "end_time": "2023-01-11T13:25:53.901827",
     "exception": false,
     "start_time": "2023-01-11T13:25:53.890112",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Titanic - Machine Learning from Disaster\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/6/6e/St%C3%B6wer_Titanic.jpg)\n",
    "\n",
    "***The RMS Titanic sank in the early morning hours of 15 April 1912 in the North Atlantic Ocean, four days into her maiden voyage from Southampton to New York City. The largest ocean liner in service at the time, Titanic had an estimated 2,224 people on board when she struck an iceberg at around 23:40 (ship's time) on Sunday, 14 April 1912. Her sinking two hours and forty minutes later at 02:20 (ship's time; 05:18 GMT) on Monday, 15 April, resulted in the deaths of more than 1,500 people, making it one of the deadliest peacetime maritime disasters in history.*** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc51cbb",
   "metadata": {
    "papermill": {
     "duration": 0.010773,
     "end_time": "2023-01-11T13:25:53.923414",
     "exception": false,
     "start_time": "2023-01-11T13:25:53.912641",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Table of Contents\n",
    "\n",
    "* [Introduction](#introduction)\n",
    "* [House Keeping](#house)\n",
    "* [Exploratory Data Analysis](#EDA)\n",
    "* [Feature Selection](#feature)\n",
    "* [Final Processing](#final)\n",
    "* [Modelling](#model)\n",
    "* [Model Tuning - Hyperparameter GridSearch](#tuning)\n",
    "* [Model Performance](#performance)\n",
    "* [To Do in future versions!](#future)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21555d6c",
   "metadata": {
    "papermill": {
     "duration": 0.010583,
     "end_time": "2023-01-11T13:25:53.945728",
     "exception": false,
     "start_time": "2023-01-11T13:25:53.935145",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introduction <a id=\"introduction\"></a>\n",
    "\n",
    "Analysis, Feature Engineering and Modelling of the titanic dataset from [Kaggle](https://www.kaggle.com/competitions/titanic/overview).\n",
    "\n",
    "In this notebook is my first attempt of a thorough analysis of the titanic dataset. The goal was to predict survivors of the tragic sinking of the titanic based passenger information such as age, sex and socio-economic status.\n",
    "\n",
    "I tried several models, both with and without tuning to both improve my result and learn along the way.\n",
    "\n",
    "**Best performing model: 83.4%**\n",
    "\n",
    "**Hope you enjoy, let me know how I can improve, and if you liked it, an upvote would help me out alot!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927e2091",
   "metadata": {
    "papermill": {
     "duration": 0.0105,
     "end_time": "2023-01-11T13:25:53.967061",
     "exception": false,
     "start_time": "2023-01-11T13:25:53.956561",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Columns in the dataset\n",
    "\n",
    "The columns present in the dataset are as follows: \n",
    "1. **PassengerId**: This column assigns a unique identifier for each passenger.\n",
    "2. **Survived**: Specifies whether the given passenger survived or not (1 - survived, 0 - didn't survive)\n",
    "3. **Pclass**: The passenger's class. (1 = Upper Deck, 2 = Middle Deck, 3 = Lower Deck)\n",
    "4. **Name**: The name of the passenger. \n",
    "5. **Sex**: The sex of the passenger (male, female)\n",
    "6. **Age**: The age of the passenger in years. If the age is estimated, is it in the form of xx.5. \n",
    "7. **SibSp**: How many siblings or spouses the passenger had on board with them. Sibling = brother, sister, stepbrother, stepsister and Spouse = husband, wife (mistresses and fianc√©s were ignored)\n",
    "8. **Parch**: How many parents or children the passenger had on boad with them. Parent = mother and father, child = daughter, son, stepdaughter and stepson and some children travelled only with a nanny, therefore parch=0 for them.\n",
    "9. **Ticket**: The ticket of the passenger. \n",
    "10. **Fare**: The fare amount paid by the passenger for the trip. \n",
    "11. **Cabin**: The cabin in which the passenger stayed. \n",
    "12. **Embarked**: The place from which the passenger embarked (S, C, Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6dc7df",
   "metadata": {
    "papermill": {
     "duration": 0.010377,
     "end_time": "2023-01-11T13:25:53.988190",
     "exception": false,
     "start_time": "2023-01-11T13:25:53.977813",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# House Keeping <a id=\"house\"></a>\n",
    "\n",
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e674977",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T13:25:54.011944Z",
     "iopub.status.busy": "2023-01-11T13:25:54.011258Z",
     "iopub.status.idle": "2023-01-11T13:25:55.516938Z",
     "shell.execute_reply": "2023-01-11T13:25:55.515668Z"
    },
    "papermill": {
     "duration": 1.520812,
     "end_time": "2023-01-11T13:25:55.519774",
     "exception": false,
     "start_time": "2023-01-11T13:25:53.998962",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import FileLink # add stuff to file\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_palette(\"YlGnBu\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV \n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33b4834",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-06T13:31:47.419917Z",
     "iopub.status.busy": "2023-01-06T13:31:47.419380Z",
     "iopub.status.idle": "2023-01-06T13:31:47.425184Z",
     "shell.execute_reply": "2023-01-06T13:31:47.424192Z",
     "shell.execute_reply.started": "2023-01-06T13:31:47.419872Z"
    },
    "papermill": {
     "duration": 0.01113,
     "end_time": "2023-01-11T13:25:55.543090",
     "exception": false,
     "start_time": "2023-01-11T13:25:55.531960",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataset load and summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8ecf774",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T13:25:55.568976Z",
     "iopub.status.busy": "2023-01-11T13:25:55.568632Z",
     "iopub.status.idle": "2023-01-11T13:25:55.614429Z",
     "shell.execute_reply": "2023-01-11T13:25:55.613113Z"
    },
    "papermill": {
     "duration": 0.062434,
     "end_time": "2023-01-11T13:25:55.616765",
     "exception": false,
     "start_time": "2023-01-11T13:25:55.554331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Dataset:\n",
      "Loaded train dataset with shape (891, 13) (891 rows and 13 columns)\n",
      "\n",
      "Test Dataset:\n",
      "Loaded test dataset with shape (418, 13) (418 rows and 13 columns)\n",
      "\n",
      "Sample Submission Dataset:\n",
      "Loaded sample submission dataset with shape (418, 2) (418 rows and 2 columns)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load datasets\n",
    "df_train = pd.read_csv('/kaggle/input/titanic/train.csv')\n",
    "df_test = pd.read_csv('/kaggle/input/titanic/test.csv')\n",
    "df_gender_submission = pd.read_csv('/kaggle/input/titanic/gender_submission.csv')\n",
    "\n",
    "# mark train and test sets for future split\n",
    "df_train['train_test'] = 1\n",
    "df_test['train_test'] = 0\n",
    "df_test['Survived'] = np.NaN\n",
    "\n",
    "#combine to a single dataframe with all data for feature engineering\n",
    "df_all = pd.concat((df_train, df_test))\n",
    "\n",
    "# print dataset shape and columns\n",
    "print(f'''\n",
    "Train Dataset:\n",
    "Loaded train dataset with shape {df_train.shape} ({df_train.shape[0]} rows and {df_train.shape[1]} columns)\n",
    "\n",
    "Test Dataset:\n",
    "Loaded test dataset with shape {df_test.shape} ({df_test.shape[0]} rows and {df_test.shape[1]} columns)\n",
    "\n",
    "Sample Submission Dataset:\n",
    "Loaded sample submission dataset with shape {df_gender_submission.shape} ({df_gender_submission.shape[0]} rows and {df_gender_submission.shape[1]} columns)\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b7447a",
   "metadata": {
    "papermill": {
     "duration": 0.010767,
     "end_time": "2023-01-11T13:25:55.641107",
     "exception": false,
     "start_time": "2023-01-11T13:25:55.630340",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aacb67c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T13:25:55.664977Z",
     "iopub.status.busy": "2023-01-11T13:25:55.664436Z",
     "iopub.status.idle": "2023-01-11T13:25:55.684477Z",
     "shell.execute_reply": "2023-01-11T13:25:55.683405Z"
    },
    "papermill": {
     "duration": 0.034246,
     "end_time": "2023-01-11T13:25:55.686402",
     "exception": false,
     "start_time": "2023-01-11T13:25:55.652156",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>train_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  train_test  \n",
       "0      0         A/5 21171   7.2500   NaN        S           1  \n",
       "1      0          PC 17599  71.2833   C85        C           1  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S           1  \n",
       "3      0            113803  53.1000  C123        S           1  \n",
       "4      0            373450   8.0500   NaN        S           1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c5a819f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T13:25:55.710894Z",
     "iopub.status.busy": "2023-01-11T13:25:55.710540Z",
     "iopub.status.idle": "2023-01-11T13:25:55.730771Z",
     "shell.execute_reply": "2023-01-11T13:25:55.729545Z"
    },
    "papermill": {
     "duration": 0.03503,
     "end_time": "2023-01-11T13:25:55.733366",
     "exception": false,
     "start_time": "2023-01-11T13:25:55.698336",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 13 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      " 12  train_test   891 non-null    int64  \n",
      "dtypes: float64(2), int64(6), object(5)\n",
      "memory usage: 90.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46957bdc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T13:25:55.757437Z",
     "iopub.status.busy": "2023-01-11T13:25:55.757085Z",
     "iopub.status.idle": "2023-01-11T13:25:55.790151Z",
     "shell.execute_reply": "2023-01-11T13:25:55.788694Z"
    },
    "papermill": {
     "duration": 0.047764,
     "end_time": "2023-01-11T13:25:55.792417",
     "exception": false,
     "start_time": "2023-01-11T13:25:55.744653",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>train_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>32.204208</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>257.353842</td>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>14.526497</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>49.693429</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>223.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>20.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.910400</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>668.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
       "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
       "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
       "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
       "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
       "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
       "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
       "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
       "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
       "\n",
       "            Parch        Fare  train_test  \n",
       "count  891.000000  891.000000       891.0  \n",
       "mean     0.381594   32.204208         1.0  \n",
       "std      0.806057   49.693429         0.0  \n",
       "min      0.000000    0.000000         1.0  \n",
       "25%      0.000000    7.910400         1.0  \n",
       "50%      0.000000   14.454200         1.0  \n",
       "75%      0.000000   31.000000         1.0  \n",
       "max      6.000000  512.329200         1.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47071d1",
   "metadata": {
    "papermill": {
     "duration": 0.011114,
     "end_time": "2023-01-11T13:25:55.815065",
     "exception": false,
     "start_time": "2023-01-11T13:25:55.803951",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Initial thoughs\n",
    "\n",
    "* The **PassengerID** column shouldnt provide any useful information about survival, so it should be dropped.\n",
    "* The **Fare** column looks very volatile on the high end, Q3 (75%) = 31 and MAX = 512, maybe outliers?\n",
    "* There are null values in **Age**, **Cabin** and **Embarked**, these should be fixed, maybe **Age** and **Embarked** are missing data and **Cabin** is simply due to not every passenger having a **Cabin**\n",
    "* Both numerical and categorical columns. They should be examined further and either scaled or one hot encoded to improve model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724ae9bf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.011094,
     "end_time": "2023-01-11T13:25:55.837575",
     "exception": false,
     "start_time": "2023-01-11T13:25:55.826481",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Exploratory Data Analysis <a id=\"EDA\"></a>\n",
    "\n",
    "## Survival ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73a02c3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T13:25:55.861767Z",
     "iopub.status.busy": "2023-01-11T13:25:55.861452Z",
     "iopub.status.idle": "2023-01-11T13:25:55.952786Z",
     "shell.execute_reply": "2023-01-11T13:25:55.951310Z"
    },
    "papermill": {
     "duration": 0.10553,
     "end_time": "2023-01-11T13:25:55.954547",
     "exception": true,
     "start_time": "2023-01-11T13:25:55.849017",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_survived' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19/1710742843.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_survived\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_survived' is not defined"
     ]
    }
   ],
   "source": [
    "df_survived.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf968e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T12:24:12.299615Z",
     "iopub.status.busy": "2023-01-11T12:24:12.298591Z",
     "iopub.status.idle": "2023-01-11T12:24:12.410743Z",
     "shell.execute_reply": "2023-01-11T12:24:12.409919Z",
     "shell.execute_reply.started": "2023-01-11T12:24:12.299574Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_survived = df_train['Survived']\n",
    "\n",
    "survival, mortality = df_survived.value_counts() \n",
    "\n",
    "print(f'''\n",
    "There were {survival} survivors and {mortality} mortalities in the train set.\n",
    "Making the survival rate {df_survived.mean():.2%}\n",
    "''')\n",
    "\n",
    "sns.countplot(x = df_survived)\n",
    "plt.title('Distribution of survival or mortality')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d19d11e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e7aa3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-08T15:07:32.292796Z",
     "iopub.status.busy": "2023-01-08T15:07:32.291183Z",
     "iopub.status.idle": "2023-01-08T15:07:32.317424Z",
     "shell.execute_reply": "2023-01-08T15:07:32.315726Z",
     "shell.execute_reply.started": "2023-01-08T15:07:32.292746Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract titles\n",
    "df_train['Title'] = df_train['Name'].str.split(',', expand=True)[1]\n",
    "                                    .str.split('.', expand=True)[0]\n",
    "\n",
    "# List most frequent titles\n",
    "(df_train['Title'].value_counts()\n",
    "                  .to_frame()\n",
    "                  .reset_index()\n",
    "                  .iloc[:6]\n",
    "                  .rename(columns={'index':'Title', 'Title':'Frequency'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4614545",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Ticket Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1a6662",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T12:26:03.003602Z",
     "iopub.status.busy": "2023-01-11T12:26:03.003154Z",
     "iopub.status.idle": "2023-01-11T12:26:03.140758Z",
     "shell.execute_reply": "2023-01-11T12:26:03.139810Z",
     "shell.execute_reply.started": "2023-01-11T12:26:03.003567Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_pclass = df_train['Pclass']\n",
    "\n",
    "upper, middle, lower = df_pclass.value_counts()\n",
    "\n",
    "print(f'''\n",
    "Passengers were split into three Ticket Classes and hereby the placement on the ship deck:\n",
    "There were {upper} people on the upper deck.\n",
    "There were {middle} people on the middle deck.\n",
    "There were {lower} people on the lower deck\n",
    "''')\n",
    "\n",
    "sns.countplot(x = df_pclass)\n",
    "plt.title('Distribution of ticket classes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842c6348",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Passenger sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf9ee41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T12:27:19.318425Z",
     "iopub.status.busy": "2023-01-11T12:27:19.318033Z",
     "iopub.status.idle": "2023-01-11T12:27:19.449521Z",
     "shell.execute_reply": "2023-01-11T12:27:19.448746Z",
     "shell.execute_reply.started": "2023-01-11T12:27:19.318392Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_sex = df_train['Sex']\n",
    "\n",
    "male, female = df_sex.value_counts().sort_index()\n",
    "\n",
    "print(f'''\n",
    "There were {male} males aboard.\n",
    "There were {female} females aboard.\n",
    "''') \n",
    "\n",
    "sns.countplot(x = df_sex)\n",
    "plt.title('Distribution of passenger sex')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39da8ff2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Passenger age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92956c32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-08T15:09:21.112296Z",
     "iopub.status.busy": "2023-01-08T15:09:21.111375Z",
     "iopub.status.idle": "2023-01-08T15:09:21.377515Z",
     "shell.execute_reply": "2023-01-08T15:09:21.376279Z",
     "shell.execute_reply.started": "2023-01-08T15:09:21.112257Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_age = df_train['Age']\n",
    "\n",
    "print(f'''\n",
    "There were {np.count_nonzero(df_age < 25)} passenges under the age of 25.\n",
    "There were {np.count_nonzero((df_age >= 25) & (df_age <= 65))} passengers between the age of 25 and 65.\n",
    "There were {np.count_nonzero(df_age > 65)} passenges older than 65.\n",
    "''') \n",
    "\n",
    "sns.histplot(data = df_age)\n",
    "plt.title('Distribution of passenger age')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097d4554",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Number of siblings/spouses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5b5074",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-06T20:52:53.649680Z",
     "iopub.status.busy": "2023-01-06T20:52:53.649344Z",
     "iopub.status.idle": "2023-01-06T20:52:53.808936Z",
     "shell.execute_reply": "2023-01-06T20:52:53.808083Z",
     "shell.execute_reply.started": "2023-01-06T20:52:53.649649Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_sibsp = df_train['SibSp']\n",
    "\n",
    "print(f'There were {df_sibsp.value_counts().sort_index()[0]} passengers with no siblings or spouses.')\n",
    "\n",
    "sns.countplot(x = df_sibsp)\n",
    "plt.title('Distribution of number of siblings/spouses aboard')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b9171b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Number of parents/children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a842805",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-06T20:52:53.810643Z",
     "iopub.status.busy": "2023-01-06T20:52:53.810127Z",
     "iopub.status.idle": "2023-01-06T20:52:53.972836Z",
     "shell.execute_reply": "2023-01-06T20:52:53.971924Z",
     "shell.execute_reply.started": "2023-01-06T20:52:53.810611Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_parch = df_train['Parch']\n",
    "\n",
    "print(f'There were {df_parch.value_counts().sort_index()[0]} passengers with no parents or children.')\n",
    "\n",
    "sns.countplot(x = df_parch)\n",
    "plt.title('Distribution of number of parents/children aboard')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c190834",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Tickets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4429a654",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-08T15:18:17.075841Z",
     "iopub.status.busy": "2023-01-08T15:18:17.074403Z",
     "iopub.status.idle": "2023-01-08T15:18:17.323387Z",
     "shell.execute_reply": "2023-01-08T15:18:17.322143Z",
     "shell.execute_reply.started": "2023-01-08T15:18:17.075787Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_ticket = df_train['Ticket']\n",
    "\n",
    "\n",
    "\n",
    "print(f'''\n",
    "There were {np.count_nonzero(df_ticket.value_counts() == 1)} passengers who bought their ticket alone.\n",
    "There were {np.count_nonzero(df_ticket.value_counts() > 1)} passengers who bought tickets together.\n",
    "''') \n",
    "\n",
    "sns.histplot(data = df_ticket.value_counts())\n",
    "plt.title('Distribution of people per ticket')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2025c845",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ca67c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-08T15:19:02.434960Z",
     "iopub.status.busy": "2023-01-08T15:19:02.434443Z",
     "iopub.status.idle": "2023-01-08T15:19:02.833781Z",
     "shell.execute_reply": "2023-01-08T15:19:02.832592Z",
     "shell.execute_reply.started": "2023-01-08T15:19:02.434921Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_fare = df_train['Fare']\n",
    "\n",
    "print(f'''\n",
    "There were {np.count_nonzero(df_fare < 10)} passengers payed less than 10 dollars for their ticket.\n",
    "There were {np.count_nonzero((df_fare >= 10) & (df_fare <= 50))} passengers payed between 10 and 50 dollars for their ticket.\n",
    "There were {np.count_nonzero(df_fare > 50)} passengers payed more than 50 dollars for their ticket.\n",
    "''') \n",
    "\n",
    "\n",
    "sns.histplot(data = df_fare)\n",
    "plt.title('Distribution of fares')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9447e8e9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Cabin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a30bec4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T12:40:24.139820Z",
     "iopub.status.busy": "2023-01-11T12:40:24.139448Z",
     "iopub.status.idle": "2023-01-11T12:40:24.249345Z",
     "shell.execute_reply": "2023-01-11T12:40:24.248494Z",
     "shell.execute_reply.started": "2023-01-11T12:40:24.139792Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_cabin = df_train['Cabin']\n",
    "\n",
    "print(f'''\n",
    "There were {df_cabin.notna().astype(int).sum()} passengers who had a cabin. \n",
    "There were {df_cabin.isna().astype(int).sum()} passengers who did not have a cabin.\n",
    "''') \n",
    "\n",
    "df_cabin = np.where(df_cabin.isna(), 0, 1)\n",
    "\n",
    "sns.countplot(x = df_cabin)\n",
    "plt.title('Distribution of number of passengers with a cabin')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cea869",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Port of Embarkation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1883a6b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T12:42:18.340154Z",
     "iopub.status.busy": "2023-01-11T12:42:18.339799Z",
     "iopub.status.idle": "2023-01-11T12:42:18.475331Z",
     "shell.execute_reply": "2023-01-11T12:42:18.474580Z",
     "shell.execute_reply.started": "2023-01-11T12:42:18.340123Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_port = df_train['Embarked']\n",
    "\n",
    "C, Q, S = df_port.value_counts().sort_index()\n",
    "\n",
    "print(f'''\n",
    "There were {S} passengers boarding the ship at Southampton.\n",
    "There were {C} passengers boarding the ship at Cherbourg.\n",
    "There were {Q} passengers boarding the ship at Queenstown.\n",
    "''') \n",
    "\n",
    "sns.countplot(x = df_port)\n",
    "plt.title('Distribution of number of passengers with a cabin')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1995147",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Survival rate factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a7200a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T12:42:38.891737Z",
     "iopub.status.busy": "2023-01-11T12:42:38.891356Z",
     "iopub.status.idle": "2023-01-11T12:42:39.279937Z",
     "shell.execute_reply": "2023-01-11T12:42:39.278842Z",
     "shell.execute_reply.started": "2023-01-11T12:42:38.891704Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.catplot(data=df_train, \n",
    "            x=\"Sex\", \n",
    "            y=\"Survived\", \n",
    "            hue=\"Pclass\", \n",
    "            kind=\"bar\")\n",
    "\n",
    "plt.title('Survival rate based on sex and passanger class')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3763976d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Feature Selection <a id=\"feature\"></a>\n",
    "\n",
    "One of the best ways of getting column correlations is a **confusion matrix**. A **confusion matrix** plots the correlation of every column compared to each other, returning a matrix of scores. A score between 0 and -1 indicates negative correlation and 0 and 1 indicates positive correlations with values closer to -1/1 indicating stronger correlations.\n",
    "\n",
    "\n",
    "A **heatmap** takes this a step further, adding a color scale to the values, making correlations easier to spot at a glance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c140c80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T13:04:09.374653Z",
     "iopub.status.busy": "2023-01-11T13:04:09.374273Z",
     "iopub.status.idle": "2023-01-11T13:04:09.411259Z",
     "shell.execute_reply": "2023-01-11T13:04:09.410361Z",
     "shell.execute_reply.started": "2023-01-11T13:04:09.374622Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# change cabin names and numbers to cabin yes or no\n",
    "df_train['Cabin'] = np.where(df_train['Cabin'].isna(), 0, 1)\n",
    "\n",
    "# change male/female to 0 and 1\n",
    "df_train['Sex'] = np.where(df_train['Sex'] == 'female', 1, 0)\n",
    "\n",
    "# One-Hot encode Embarkation (done with pd.get_dummies() further down)\n",
    "df_train.loc[df_train['Embarked'] == 'S', 'embarked_Southampton'] = 1\n",
    "df_train.loc[df_train['Embarked'] == 'C', 'embarked_Cherbough'] = 1\n",
    "df_train.loc[df_train['Embarked'] == 'Q', 'embarked_Queenstown'] = 1\n",
    "\n",
    "df_train = df_train.drop('Embarked', axis = 1)\n",
    "\n",
    "df_train = df_train.replace(np.nan, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3308fcda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-06T20:52:55.223994Z",
     "iopub.status.busy": "2023-01-06T20:52:55.223145Z",
     "iopub.status.idle": "2023-01-06T20:52:55.253013Z",
     "shell.execute_reply": "2023-01-06T20:52:55.250949Z",
     "shell.execute_reply.started": "2023-01-06T20:52:55.223942Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Corelation matrix of numerical categories\n",
    "(df_train[[\n",
    "          'PassengerId', \n",
    "          'Survived', \n",
    "          'Age',\n",
    "          'SibSp',\n",
    "          'Parch',\n",
    "          'Fare',\n",
    "          'Cabin',\n",
    "          'Pclass',\n",
    "          'embarked_Southampton',\n",
    "          'embarked_Cherbough',\n",
    "          'embarked_Queenstown']]\n",
    "          .corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e23260",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T12:46:32.936014Z",
     "iopub.status.busy": "2023-01-11T12:46:32.935622Z",
     "iopub.status.idle": "2023-01-11T12:46:33.695795Z",
     "shell.execute_reply": "2023-01-11T12:46:33.694783Z",
     "shell.execute_reply.started": "2023-01-11T12:46:32.935977Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Heatmap of correlation matrix for training data columns\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8)) \n",
    "\n",
    "sns.heatmap((df_train[[\n",
    "                      'PassengerId', \n",
    "                      'Survived', \n",
    "                      'Age',\n",
    "                      'SibSp',\n",
    "                      'Parch',\n",
    "                      'Fare',\n",
    "                      'Cabin',\n",
    "                      'Pclass',\n",
    "                      'embarked_Southampton',\n",
    "                      'embarked_Cherbough',\n",
    "                      'embarked_Queenstown'\n",
    "                      ]]\n",
    "                      .corr()),\n",
    "                      linewidths=1,\n",
    "                      cmap=plt.cm.Blues, \n",
    "                      annot=True,\n",
    "                      ax=ax)\n",
    "\n",
    "plt.title('Heatmap for correlation between columns of training data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b5379a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Heatmap Conclusion\n",
    "\n",
    "The **heatmap** above shows correlation between all our features. Especially interesting is the feature correlation with our label, ```Survived```. Remember that a postive number close to 1 indicates a strong, positive correlation, while a negative number close to -1 indicates a strong, negative correlation. Numbers closer to 0 indicates a weak correlation:\n",
    "\n",
    "* ```Fare``` and ```Cabin``` seems to have the strongest positive correlations with our label (**0.26** and **0.32** respectively)\n",
    "* ```Pclass``` seems to have a strong negative correlation with out label (**-0.34**). However, remember, the upper deck is encoded as 1, middel as 2 and lower as 3. This means that the negative correlation with ```Survived``` has to be put in that context: A higher value lowers survival rate, the lower you are on the ship, the lower your survival rate is, which is what we would expect.\n",
    "* Maybe a bit surprisingly, we see a positive correlation for ```embarked_Cherbough``` and a negative correlation for ```embarked_Southampton```. This is most likely due to where workers on the ship boarded the ship or the order, that the people who embarked last, were placed higher up in the ship, giving them a better survival rate.\n",
    "* ```SibSp```, ```Parch``` and ```Age``` all show a correlation, however I would, at first glance, assume that ```Age``` had larger influence over the survival rate, but the dataset doesn't support my hypothesis.\n",
    "\n",
    "Based on these correlations, we can make better decisions on which features to keep, engineer og drop from the dataset before we start modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2a5dad",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Final Processing <a id=\"final\"></a>\n",
    "\n",
    "Before we model our data, some processing are encouraged to increase model performance.\n",
    "\n",
    "There are several ways to treat different data types, but the following are often a guide first try:\n",
    "\n",
    "* **Remove null values:** Most sk.learn models dont accept null values, so these have to be fixed.\n",
    "* **OneHotEncode categorical columns:** Encode categorical features to a more machine learning friendly format. Our ```Cabin``` feature should be transformed into ```is_A```, ```is_B``` and ```is_C``` columns with 1/0 depending on which cabin the passenger had.\n",
    "* **Scale numerical data:** Numerical values on different scales function poorly in a model. A scaler scales (duh) the data to values between -1 and 1. This keep the relative different of each feature, but allows much better model performance with several features.\n",
    "\n",
    "Next our features can drastically improve model performance. Feature engineering differs based on approach, dataset and model. For the Titanic dataset I have done the following but there are many more ways to improve my features:  \n",
    "\n",
    "* **Change Cabin names and numbers:** Change ```Cabin``` to 1 or 0 for either having or not having a cabin.\n",
    "* **Extract Titles:** The ```Name``` feature by itself provides little value in predicting wether a passanger survived or not. Maybe extracting the titles from ```Name``` provides better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d99759a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-08T14:51:23.613917Z",
     "iopub.status.busy": "2023-01-08T14:51:23.613444Z",
     "iopub.status.idle": "2023-01-08T14:51:23.643242Z",
     "shell.execute_reply": "2023-01-08T14:51:23.641913Z",
     "shell.execute_reply.started": "2023-01-08T14:51:23.613883Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312bd728",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T19:48:04.875500Z",
     "iopub.status.busy": "2023-01-09T19:48:04.875044Z",
     "iopub.status.idle": "2023-01-09T19:48:04.924658Z",
     "shell.execute_reply": "2023-01-09T19:48:04.923482Z",
     "shell.execute_reply.started": "2023-01-09T19:48:04.875458Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# impute nulls for continuous data \n",
    "df_all.Age = df_all.Age.fillna(df_train.Age.median())\n",
    "df_all.Fare = df_all.Fare.fillna(df_train.Fare.median())\n",
    "\n",
    "# drop null the two null Embarked values\n",
    "df_all.dropna(subset=['Embarked'], inplace = True)\n",
    "\n",
    "# change cabin names and numbers to cabin yes or no\n",
    "df_all['Cabin'] = np.where(df_all['Cabin'].isna(), 0, 1)\n",
    "\n",
    "# extract titles\n",
    "df_all['Title'] = (df_all['Name'].str.split(',', expand=True)[1]\n",
    "                                 .str.split('.', expand=True)[0])\n",
    "\n",
    "# drop unneeded columns\n",
    "df_all = (df_all.drop([\n",
    "                      'PassengerId',\n",
    "                      'Name',\n",
    "                      'Title',\n",
    "                      'Ticket',\n",
    "                      'Name'],\n",
    "                      axis = 1\n",
    "                      ))\n",
    "\n",
    "df_all['Pclass'] = df_all['Pclass'].astype(str)\n",
    "\n",
    "# make dummies (OneHotEncode) categorical variables\n",
    "df_all_dummies = pd.get_dummies(df_all[['Pclass', \n",
    "                                        'Sex', \n",
    "                                        'Age',\n",
    "                                        'SibSp', \n",
    "                                        'Parch', \n",
    "                                        'Fare', \n",
    "                                        'Cabin', \n",
    "                                        'Embarked', \n",
    "                                        'train_test']])\n",
    "\n",
    "# Scale data\n",
    "scaler = StandardScaler()\n",
    "(df_all_dummies[['Age', \n",
    "                 'SibSp', \n",
    "                 'Parch', \n",
    "                 'Fare']]) = (scaler.fit_transform(df_all_dummies[['Age', \n",
    "                                                                   'SibSp', \n",
    "                                                                   'Parch', \n",
    "                                                                   'Fare']]))\n",
    "\n",
    "# resplit into train and test sets\n",
    "X_train = df_all_dummies[df_all_dummies.train_test == 1].drop(['train_test'], axis =1)\n",
    "X_test = df_all_dummies[df_all_dummies.train_test == 0].drop(['train_test'], axis =1)\n",
    "y_train = df_all[df_all['train_test'] == 1]['Survived']\n",
    "y_test = df_all[df_all['train_test'] == 0]['Survived']\n",
    "\n",
    "print(f'Before training models our train set has {X_train.shape} rows and columns and our test set has {X_test.shape} rows and columns.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48d712f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-08T16:27:08.237173Z",
     "iopub.status.busy": "2023-01-08T16:27:08.236683Z",
     "iopub.status.idle": "2023-01-08T16:27:08.245358Z",
     "shell.execute_reply": "2023-01-08T16:27:08.243473Z",
     "shell.execute_reply.started": "2023-01-08T16:27:08.237138Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Modelling <a id=\"model\"></a>\n",
    "\n",
    "We have a preprocessed and model ready dataset, now to chose the right model. We are trying to predict which passangers survived the titanic sinkage, so most of the models we are testing, are **classification** models.\n",
    "\n",
    "To get an indication of the best model, I will try several, baseline, models without any tuning. After getting an indication of which model performs best on our dataset, I will use grid search to tune the model hyperparameters to further improve the accuracy.\n",
    "\n",
    "* [Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html) - Naive Bayes models uses Bayes Theorem that offers conditional probability of events taking place.\n",
    "* [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "* [Decision Tree Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) - Decision Trees create a series of decisions to classify data based on the rules learned from the dataset.\n",
    "* [KNeighbors Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) - Neightbor Classifiers groups data with other data near to it based in a specified k value.\n",
    "* [Random Forest Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) - Random Forest classifiers fits a number of decision treees on subsamples of the dataset to improve the accuracy and redude over-fitting.\n",
    "* [Support Vector Classifier (SVC)](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)\n",
    "* [XGBoost Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) - Bootsting Classifiers builds additive models to allow optimization of the downstream models based on loss functions.\n",
    "* [Voting Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html) - Voting Classifier trains different models using the chosen algorithms, returning the majority's vote as the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07701ae2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T19:54:17.333522Z",
     "iopub.status.busy": "2023-01-09T19:54:17.333141Z",
     "iopub.status.idle": "2023-01-09T19:54:22.119692Z",
     "shell.execute_reply": "2023-01-09T19:54:22.118627Z",
     "shell.execute_reply.started": "2023-01-09T19:54:17.333492Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "cv = cross_val_score(gnb, \n",
    "                     X_train, \n",
    "                     y_train, \n",
    "                     cv=5)\n",
    "\n",
    "print(f'GaussianNB: \\n{cv}\\nAverage: {cv.mean()}\\n')\n",
    "\n",
    "lr = LogisticRegression(max_iter=2000)\n",
    "cv = cross_val_score(lr, \n",
    "                     X_train, \n",
    "                     y_train, \n",
    "                     cv=5)\n",
    "\n",
    "print(f'LogisticRegression: \\n{cv}\\nAverage: {cv.mean()}\\n')\n",
    "\n",
    "dt = tree.DecisionTreeClassifier(random_state=42)\n",
    "cv = cross_val_score(dt, \n",
    "                     X_train, \n",
    "                     y_train, \n",
    "                     cv=5)\n",
    "\n",
    "print(f'DecisionTreeClassifier: \\n{cv}\\nAverage: {cv.mean()}\\n')\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "cv = cross_val_score(knn, \n",
    "                     X_train, \n",
    "                     y_train, \n",
    "                     cv=5)\n",
    "\n",
    "print(f'KNeighborsClassifier: \\n{cv}\\nAverage: {cv.mean()}\\n')\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "cv = cross_val_score(rf, \n",
    "                     X_train, \n",
    "                     y_train, \n",
    "                     cv=5)\n",
    "\n",
    "print(f'RandomForestClassifier: \\n{cv}\\nAverage: {cv.mean()}\\n')\n",
    "\n",
    "svc = SVC(probability=True)\n",
    "cv = cross_val_score(svc, \n",
    "                     X_train, \n",
    "                     y_train, \n",
    "                     cv=5)\n",
    "\n",
    "print(f'SVC: \\n{cv}\\nAverage: {cv.mean()}\\n')\n",
    "\n",
    "xgb = XGBClassifier(random_state = 42)\n",
    "cv = cross_val_score(xgb, \n",
    "                     X_train, \n",
    "                     y_train, \n",
    "                     cv=5)\n",
    "\n",
    "print(f'XGBClassifier: \\n{cv}\\nAverage: {cv.mean()}\\n')\n",
    "\n",
    "voting_clf = VotingClassifier(estimators=[\n",
    "                                          ('lr', lr), \n",
    "                                          ('knn', knn), \n",
    "                                          ('rf', rf), \n",
    "                                          ('gnb', gnb), \n",
    "                                          ('dt', dt), \n",
    "                                          ('svc', svc), \n",
    "                                          ('xgb', xgb)],\n",
    "                                          voting='soft'\n",
    "                                          )\n",
    "cv = cross_val_score(voting_clf, X_train, y_train, cv=5)\n",
    "\n",
    "print(f'VotingClassifier: \\n{cv}\\nAverage: {cv.mean()}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba4e624",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Baseline submission of best performing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10c1b65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-10T14:05:23.472873Z",
     "iopub.status.busy": "2023-01-10T14:05:23.472467Z",
     "iopub.status.idle": "2023-01-10T14:05:23.491124Z",
     "shell.execute_reply": "2023-01-10T14:05:23.489755Z",
     "shell.execute_reply.started": "2023-01-10T14:05:23.472841Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "y_hat_baseline = voting_clf.predict(X_test).astype(int)\n",
    "\n",
    "baseline_submission = pd.DataFrame({'PassengerId': df_test.PassengerId, \n",
    "                                    'Survived': y_hat_baseline})\n",
    "\n",
    "baseline_submission.to_csv('baseline_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d87ba5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Baseline model performance\n",
    "\n",
    "|Model|Baseline Performance|\n",
    "|--|--|\n",
    "|Naive Bayes| 77.0%|\n",
    "|Logistic Regression| 80.5%| \n",
    "|Decision Tree Classifier| 78.4%|\n",
    "|KNN Classifier| 80.5%|\n",
    "|Random Forest Classifier| 80.5%|\n",
    "|**Support Vector Classifier**| **82.5%**|\n",
    "|Xtreme Gradient Boosting| 82.2%|\n",
    "|Voting Classifier| 82.0%|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eafaa75",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "The **Support Vector Classifier** performed best before any model tuning. Next up, hyperparameter tuning using GridSearch. GridSearch will search the ```parameter_grid``` for each possible combination of parameter to find the best performing model. It's like running the model alot of times, searching for the best parameter combination. Some models, like **Naive Bayes**, has very few hyperparameters to tune, so they wont be tuned. \n",
    "\n",
    "A note on the **Random Tree Classifier**: Since the ```parameter_grid``` is so large, a GridSearch looking for all possible combinations, would take way too long to train. One way of dealing with this issue, is it first run a RandomizedGridSearch, which looks at the same ```parameter_grid```, but instead of looking at all combinations, it will check random parameter combinations. This drastically reduces the training time, but may miss the best combination of parameters. The best parameter combination from the RandomSearchCV is then used to guide a more narrow GridSearch, to find the best combination. This two-grid search approach can prove more realistic when then hyperparameter space is very large, like in a **Random Tree** model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0beb5b12",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Model Tuning - Hyperparameter GridSearch <a id=\"tuning\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e0a45f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T20:03:06.916289Z",
     "iopub.status.busy": "2023-01-09T20:03:06.915920Z",
     "iopub.status.idle": "2023-01-09T20:03:09.715351Z",
     "shell.execute_reply": "2023-01-09T20:03:09.714665Z",
     "shell.execute_reply.started": "2023-01-09T20:03:06.916256Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def model_performance(model, name):\n",
    "    print(name)\n",
    "    print(f'Best Score: {model.best_score_}')\n",
    "    print(f'Best Parameters: {model.best_params_}\\n')  \n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "parameter_grid = {'max_iter' : [2000],\n",
    "                  'penalty' : ['l1', 'l2'],\n",
    "                  'C' : np.logspace(-4, 4, 20),\n",
    "                  'solver' : ['liblinear'\n",
    "                  ]}\n",
    "\n",
    "lr_model = GridSearchCV(lr, \n",
    "                        param_grid=parameter_grid, \n",
    "                        cv=5, \n",
    "                        verbose=True, \n",
    "                        n_jobs=1)\n",
    "best_lr_model = lr_model.fit(X_train, y_train)\n",
    "model_performance(best_lr_model, 'LogisticRegression')\n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "parameter_grid = {'n_neighbors' : [3,5,7,9],\n",
    "                  'weights' : ['uniform', 'distance'],\n",
    "                  'algorithm' : ['auto', 'ball_tree','kd_tree'],\n",
    "                  'p' : [1,2]}\n",
    "\n",
    "knn_model = GridSearchCV(knn, \n",
    "                         param_grid=parameter_grid, \n",
    "                         cv=5, \n",
    "                         verbose=True, \n",
    "                         n_jobs=-1)\n",
    "best_knn_model = knn_model.fit(X_train, y_train)\n",
    "model_performance(best_knn_model, 'KNN')\n",
    "\n",
    "\n",
    "svc = SVC(probability=True)\n",
    "\n",
    "parameter_grid = {'kernel': ['rbf'], \n",
    "                   'gamma': [.1,.5,1],\n",
    "                   'C': [.1, 1, 10]}\n",
    "\n",
    "svc_model = GridSearchCV(svc, \n",
    "                       param_grid=parameter_grid, \n",
    "                       cv=5, \n",
    "                       verbose=True, \n",
    "                       n_jobs=-1)\n",
    "best_svc_model = svc_model.fit(X_train, y_train)\n",
    "model_performance(best_svc_model,'SVC')\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "parameter_grid = {'n_estimators': [100,500], \n",
    "                  'bootstrap': [True,False],\n",
    "                  'max_depth': [10,20,50,75,None],\n",
    "                  'max_features': ['auto','sqrt'],\n",
    "                  'min_samples_leaf': [1,2,4],\n",
    "                  'min_samples_split': [2,5,10]}\n",
    "                                  \n",
    "rf_model_randomcv = RandomizedSearchCV(rf, \n",
    "                                param_distributions=parameter_grid, \n",
    "                                n_iter=50, \n",
    "                                cv=5, \n",
    "                                verbose=True, \n",
    "                                n_jobs=-1)\n",
    "best_rf_model_randomcv = rf_model_randomcv.fit(X_train, y_train)\n",
    "model_performance(best_rf_model_randomcv, 'Random Forest (RandSearchCV)')\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "parameter_grid = {'n_estimators': [500,550,600],\n",
    "              'criterion':['gini'],\n",
    "              'bootstrap': [True],\n",
    "              'max_depth': [10, 15, 20],\n",
    "              'max_features': ['auto','sqrt', 10],\n",
    "              'min_samples_leaf': [2,3],\n",
    "              'min_samples_split': [2,3]}\n",
    "                                  \n",
    "rf_model = GridSearchCV(rf, \n",
    "                      param_grid = parameter_grid, \n",
    "                      cv=5, \n",
    "                      verbose=True, \n",
    "                      n_jobs=-1)\n",
    "best_rf_model = rf_model.fit(X_train, y_train)\n",
    "model_performance(best_rf_model, 'Random Forest (GridSearchCV)')\n",
    "\n",
    "\n",
    "best_rf_model.fit(X_train, y_train)\n",
    "\n",
    "y_hat_tuned = best_rf_model.predict(X_test).astype(int)\n",
    "\n",
    "tuned_submission = pd.DataFrame({'PassengerId': df_test.PassengerId, \n",
    "                                 'Survived': y_hat_tuned})\n",
    "\n",
    "tuned_submission.to_csv('tuned_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39ecc58",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Model performance <a id=\"performance\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dc3a62",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "|Model|Scaled Performance|Scaled and Tuned Performance|\n",
    "|--|--|--|\n",
    "|Naive Bayes| 77.0%| NA|\n",
    "|Logistic Regression| 80.5%| 80.7%|\n",
    "|Decision Tree Classifier| 78.4%| NA|\n",
    "|KNN Classifier| 80.5%|80.7%|\n",
    "|**Random Forest Classifier**| 80.5%| **83.2%**|\n",
    "|Support Vector Classifier| **82.5%**| 82.9%|\n",
    "|Xtreme Gradient Boosting| 82.2%| 81.0%|\n",
    "|Voting Classifier| 82.0%| NA|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1eaa7a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Wow! Tuning the hyperparameters really improved the model performance, especially for the **Random Forest Classifier.**\n",
    "\n",
    "There is plenty more to try with the modelling, but we will accept this for now.\n",
    "\n",
    "**Feel free to try other models and let me know!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1a7a56",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e785bf",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Submission of the baseline and tuned model. Both submission .csv files are made to match the sample submission file with ```PassengerID``` and ```Survived``` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148c8428",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-10T14:05:14.449113Z",
     "iopub.status.busy": "2023-01-10T14:05:14.448600Z",
     "iopub.status.idle": "2023-01-10T14:05:14.546585Z",
     "shell.execute_reply": "2023-01-10T14:05:14.545051Z",
     "shell.execute_reply.started": "2023-01-10T14:05:14.449013Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "baseline_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18687af3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tuned_submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9b95fc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# To do in future versions! <a id=\"future\"></a>\n",
    "\n",
    "* Exploratory Data Analysis:\n",
    "    * Perform further EDA\n",
    "* Feature Engineering:\n",
    "    * Group and bin age and fare features\n",
    "    * Fare/Ticket# Column\n",
    "    * Remove fare outliers\n",
    "    * SMOTE Survival rates?\n",
    "* Model Evaluation:\n",
    "    * Make classification_report and ROC-curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285993f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-06T21:03:03.633652Z",
     "iopub.status.busy": "2023-01-06T21:03:03.632763Z",
     "iopub.status.idle": "2023-01-06T21:03:03.641332Z",
     "shell.execute_reply": "2023-01-06T21:03:03.638918Z",
     "shell.execute_reply.started": "2023-01-06T21:03:03.633611Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%%capture --no-display\n",
    "#xgb = XGBClassifier(random_state=42)\n",
    "\n",
    "#parametere_grid = {\n",
    "              #'n_estimators': [450,500,550],\n",
    "              #'colsample_bytree': [0.75,0.8,0.85],\n",
    "              #'max_depth': [None],\n",
    "              #'reg_alpha': [1],\n",
    "              #'reg_lambda': [2, 5, 10],\n",
    "              #'subsample': [0.55, 0.6, .65],\n",
    "              #'learning_rate':[0.5],\n",
    "              #'gamma':[.5,1,2],\n",
    "              #'min_child_weight':[0.01],\n",
    "              #'sampling_method': ['uniform']}\n",
    "\n",
    "#xgb_model = GridSearchCV(xgb, \n",
    "                         #param_grid=parameter_grid, \n",
    "                         #cv=5, \n",
    "                         #verbose=True, \n",
    "                         #n_jobs=-1)\n",
    "\n",
    "#best_xgb_model = xgb_model.fit(X_train, y_train)\n",
    "\n",
    "#model_performance(best_xgb_model, 'XGB')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10.12315,
   "end_time": "2023-01-11T13:25:56.690236",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-01-11T13:25:46.567086",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
